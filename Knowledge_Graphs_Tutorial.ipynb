{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tutorial-header",
   "metadata": {},
   "source": [
    "# Knowledge Graphs Tutorial: A Complete Guide to Neo4j and Cypher\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "- Understand what Knowledge Graphs are and why they're valuable\n",
    "- Learn the fundamentals of graph database concepts\n",
    "- Master basic to advanced Cypher query language\n",
    "- Build and query a movie knowledge graph using real data\n",
    "- Apply graph algorithms for data analysis\n",
    "\n",
    "## üìã Prerequisites\n",
    "- Basic understanding of databases\n",
    "- Python programming fundamentals\n",
    "- No prior Neo4j or graph database experience required!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "what-are-kgs",
   "metadata": {},
   "source": [
    "## 1. What are Knowledge Graphs?\n",
    "\n",
    "### üîç Definition\n",
    "A **Knowledge Graph** is a network of real-world entities (people, places, things, concepts) and their relationships, stored in a graph database format.\n",
    "\n",
    "### üèóÔ∏è Core Components\n",
    "1. **Nodes (Vertices)**: Represent entities (e.g., Person, Movie, Company)\n",
    "2. **Relationships (Edges)**: Connect nodes and represent how entities relate\n",
    "3. **Properties**: Attributes that describe nodes and relationships\n",
    "4. **Labels**: Categories that group similar nodes\n",
    "\n",
    "### üìä Graph vs. Traditional Databases\n",
    "\n",
    "| Aspect | Relational Database | Knowledge Graph |\n",
    "|--------|-------------------|----------------|\n",
    "| **Structure** | Tables with rows/columns | Nodes connected by relationships |\n",
    "| **Relationships** | Foreign keys, JOINs | Direct connections |\n",
    "| **Schema** | Fixed schema | Flexible, schema-optional |\n",
    "| **Queries** | SQL | Cypher, SPARQL |\n",
    "| **Best For** | Structured data, transactions | Connected data, pattern detection |\n",
    "\n",
    "### üåü Why Use Knowledge Graphs?\n",
    "- **Intuitive**: Mirrors how humans think about relationships\n",
    "- **Flexible**: Easy to add new entity types and relationships\n",
    "- **Performance**: Fast traversal of connected data\n",
    "- **Insights**: Reveals hidden patterns and connections\n",
    "- **Integration**: Combines data from multiple sources naturally\n",
    "\n",
    "### üöÄ Real-World Applications\n",
    "- **Recommendation Systems**: Netflix, Amazon product recommendations\n",
    "- **Fraud Detection**: Banking, insurance claim analysis\n",
    "- **Social Networks**: Facebook's social graph, LinkedIn connections\n",
    "- **Knowledge Management**: Google's Knowledge Graph, Wikipedia\n",
    "- **Drug Discovery**: Understanding protein interactions\n",
    "- **Supply Chain**: Tracking dependencies and risks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neo4j-intro",
   "metadata": {},
   "source": [
    "## 2. Introduction to Neo4j\n",
    "\n",
    "### üóÑÔ∏è What is Neo4j?\n",
    "Neo4j is the world's leading **native graph database**, designed from the ground up to store and query connected data efficiently.\n",
    "\n",
    "### üè∑Ô∏è Neo4j Graph Model\n",
    "- **Labeled Property Graph**: Nodes have labels, relationships have types, both can have properties\n",
    "- **ACID Compliance**: Ensures data integrity with transactions\n",
    "- **Index-Free Adjacency**: Relationships are stored as pointers, enabling fast traversals\n",
    "\n",
    "### üìù Cypher Query Language\n",
    "**Cypher** is Neo4j's declarative query language, designed to be:\n",
    "- **Visual**: Syntax resembles graph drawings\n",
    "- **Expressive**: Handles complex graph patterns\n",
    "- **Readable**: Close to natural language\n",
    "\n",
    "#### Basic Cypher Syntax Patterns:\n",
    "```cypher\n",
    "// Node pattern\n",
    "(n)           // Any node\n",
    "(n:Person)    // Node with label 'Person'\n",
    "(n:Person {name: \"Alice\"})  // Node with label and property\n",
    "\n",
    "// Relationship pattern  \n",
    "-[:KNOWS]->   // Directed relationship\n",
    "-[:KNOWS]-    // Undirected relationship\n",
    "-[r:KNOWS]->  // Relationship with variable\n",
    "\n",
    "// Complete pattern\n",
    "(alice:Person)-[:KNOWS]->(bob:Person)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-section",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "\n",
    "Let's set up our development environment and connect to Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import pandas as pd\n",
    "from langchain_community.graphs import Neo4jGraph\n",
    "\n",
    "# Warning control\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-connection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('.env', override=True)\n",
    "\n",
    "# Neo4j connection parameters\n",
    "NEO4J_URI = os.getenv('NEO4J_URI')\n",
    "NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')\n",
    "NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')\n",
    "NEO4J_DATABASE = os.getenv('NEO4J_DATABASE')\n",
    "\n",
    "# Initialize Neo4j connection\n",
    "kg = Neo4jGraph(\n",
    "    url=NEO4J_URI, \n",
    "    username=NEO4J_USERNAME, \n",
    "    password=NEO4J_PASSWORD, \n",
    "    database=NEO4J_DATABASE\n",
    ")\n",
    "\n",
    "print(\"üîå Connected to Neo4j database successfully!\")\n",
    "print(f\"üìç Database URI: {NEO4J_URI}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2zeqvh5em96",
   "source": "### üîç Neo4j Connection Health Check\\n\\nBefore we start, let's verify that your Neo4j Aura instance is running and accessible:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "9616m0a935p",
   "source": "def check_neo4j_connection():\\n    \\\"\\\"\\\"\\n    Comprehensive Neo4j connection health check\\n    \\\"\\\"\\\"\\n    print(\\\"üîç Neo4j Connection Health Check\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    # Step 1: Check environment variables\\n    print(\\\"\\\\n1Ô∏è‚É£ Environment Variables:\\\")\\n    required_vars = ['NEO4J_URI', 'NEO4J_USERNAME', 'NEO4J_PASSWORD', 'NEO4J_DATABASE']\\n    env_status = {}\\n    \\n    for var in required_vars:\\n        value = os.getenv(var)\\n        if value:\\n            # Mask sensitive information\\n            if 'PASSWORD' in var:\\n                display_value = '*' * len(value)\\n            elif 'URI' in var:\\n                display_value = value\\n            else:\\n                display_value = value\\n            print(f\\\"   ‚úÖ {var}: {display_value}\\\")\\n            env_status[var] = True\\n        else:\\n            print(f\\\"   ‚ùå {var}: Not set\\\")\\n            env_status[var] = False\\n    \\n    if not all(env_status.values()):\\n        print(\\\"\\\\n   ‚ö†Ô∏è Missing environment variables! Please check your .env file.\\\")\\n        return False\\n    \\n    # Step 2: Test basic connectivity\\n    print(\\\"\\\\n2Ô∏è‚É£ Testing Connection:\\\")\\n    try:\\n        # Simple connectivity test\\n        test_kg = Neo4jGraph(\\n            url=NEO4J_URI, \\n            username=NEO4J_USERNAME, \\n            password=NEO4J_PASSWORD, \\n            database=NEO4J_DATABASE\\n        )\\n        \\n        # Try a simple query\\n        result = test_kg.query(\\\"RETURN 'Connection successful' AS status, datetime() AS timestamp\\\")\\n        if result:\\n            print(f\\\"   ‚úÖ Connection successful!\\\")\\n            print(f\\\"   üìÖ Server timestamp: {result[0].get('timestamp', 'N/A')}\\\")\\n            return True\\n        else:\\n            print(\\\"   ‚ùå Query returned no results\\\")\\n            return False\\n            \\n    except Exception as e:\\n        print(f\\\"   ‚ùå Connection failed: {str(e)}\\\")\\n        \\n        # Provide specific guidance based on error type\\n        error_msg = str(e).lower()\\n        print(\\\"\\\\nüîß Troubleshooting Guide:\\\")\\n        \\n        if 'authentication' in error_msg or 'credentials' in error_msg:\\n            print(\\\"   ‚Ä¢ Check your username and password in .env file\\\")\\n            print(\\\"   ‚Ä¢ Verify credentials in Neo4j Aura Console\\\")\\n            \\n        elif 'connection refused' in error_msg or 'timeout' in error_msg:\\n            print(\\\"   ‚Ä¢ Your Neo4j Aura instance may be paused/stopped\\\")\\n            print(\\\"   ‚Ä¢ Go to https://console.neo4j.io to resume your instance\\\")\\n            print(\\\"   ‚Ä¢ Wait 60 seconds after resuming before retrying\\\")\\n            \\n        elif 'ssl' in error_msg or 'certificate' in error_msg:\\n            print(\\\"   ‚Ä¢ SSL/TLS connection issue\\\")\\n            print(\\\"   ‚Ä¢ Ensure your URI starts with 'neo4j+s://'\\\")\\n            \\n        elif 'database' in error_msg:\\n            print(\\\"   ‚Ä¢ Database name may be incorrect\\\")\\n            print(\\\"   ‚Ä¢ Check NEO4J_DATABASE in your .env file\\\")\\n            \\n        else:\\n            print(f\\\"   ‚Ä¢ General connection error: {str(e)[:100]}...\\\")\\n            \\n        print(\\\"\\\\nüìã Quick Fix Steps:\\\")\\n        print(\\\"   1. Go to https://console.neo4j.io\\\")\\n        print(\\\"   2. Find your Aura instance\\\")\\n        print(\\\"   3. Click 'Resume' if status shows 'Paused'\\\")\\n        print(\\\"   4. Wait 60 seconds for instance to start\\\")\\n        print(\\\"   5. Re-run this cell\\\")\\n        \\n        return False\\n\\n# Run the connection check\\nconnection_ok = check_neo4j_connection()\\n\\nif connection_ok:\\n    print(\\\"\\\\nüéâ Ready to proceed with the tutorial!\\\")\\nelse:\\n    print(\\\"\\\\n‚è∏Ô∏è Please fix the connection issues above before continuing.\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "tqitbb1o7g",
   "source": "# Only proceed with connection if health check passed\\nif 'connection_ok' not in locals() or not connection_ok:\\n    print(\\\"‚ùå Skipping connection setup - please fix connection issues first\\\")\\nelse:\\n    # Initialize Neo4j connection\\n    kg = Neo4jGraph(\\n        url=NEO4J_URI, \\n        username=NEO4J_USERNAME, \\n        password=NEO4J_PASSWORD, \\n        database=NEO4J_DATABASE\\n    )\\n    \\n    print(\\\"üîå Connected to Neo4j database successfully!\\\")\\n    print(f\\\"üìç Database URI: {NEO4J_URI}\\\")\\n    \\n    # Get basic instance information\\n    try:\\n        result = kg.query(\\\"CALL dbms.components() YIELD name, versions, edition RETURN name, versions[0] AS version, edition\\\")\\n        if result:\\n            component = result[0]\\n            print(f\\\"üìä Neo4j {component.get('edition', 'Unknown')} Edition - Version {component.get('version', 'Unknown')}\\\")\\n    except:\\n        print(\\\"üìä Instance information not available\\\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "rfttuhauvw",
   "source": "### üîÑ Retry Connection Helper\\n\\nIf your connection failed, you can use this cell to retry after fixing the issue:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "u2jelp6wam",
   "source": "# Retry connection helper - run this after fixing issues\\ndef retry_connection():\\n    print(\\\"üîÑ Retrying Neo4j connection...\\\")\\n    global kg, connection_ok\\n    \\n    # Re-run the health check\\n    connection_ok = check_neo4j_connection()\\n    \\n    if connection_ok:\\n        # Re-establish the connection\\n        kg = Neo4jGraph(\\n            url=NEO4J_URI, \\n            username=NEO4J_USERNAME, \\n            password=NEO4J_PASSWORD, \\n            database=NEO4J_DATABASE\\n        )\\n        print(\\\"\\\\n‚úÖ Connection retry successful!\\\")\\n        return True\\n    else:\\n        print(\\\"\\\\n‚ùå Connection retry failed. Please check the issues above.\\\")\\n        return False\\n\\n# Uncomment the line below to retry connection:\\n# retry_connection()\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8yvom5zl258",
   "source": "### ‚ö†Ô∏è Important Note about Neo4j Aura\\n\\n**Neo4j Aura Free instances automatically pause after 3 days of inactivity.** This is normal behavior and helps manage resources.\\n\\n#### If your instance is paused:\\n1. Go to [Neo4j Aura Console](https://console.neo4j.io)\\n2. Find your instance in the dashboard\\n3. Click the **\\\"Resume\\\"** button\\n4. Wait **60 seconds** for the instance to fully start\\n5. Re-run the connection health check above\\n\\n#### Common connection issues:\\n- **\\\"Connection refused\\\"**: Instance is paused - follow steps above\\n- **\\\"Authentication failed\\\"**: Check credentials in `.env` file\\n- **\\\"SSL/TLS errors\\\"**: Ensure URI starts with `neo4j+s://`\\n- **\\\"Database not found\\\"**: Verify database name (usually \\\"neo4j\\\")\\n\\n---\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "cypher-fundamentals",
   "metadata": {},
   "source": [
    "## 4. Cypher Fundamentals\n",
    "\n",
    "### üìö Core Cypher Clauses\n",
    "\n",
    "| Clause | Purpose | Example |\n",
    "|--------|---------|--------|\n",
    "| `MATCH` | Find patterns in the graph | `MATCH (n:Person)` |\n",
    "| `CREATE` | Create nodes and relationships | `CREATE (n:Person {name: \"Alice\"})` |\n",
    "| `MERGE` | Create if doesn't exist, match if exists | `MERGE (n:Person {name: \"Alice\"})` |\n",
    "| `SET` | Update properties | `SET n.age = 30` |\n",
    "| `DELETE` | Remove nodes/relationships | `DELETE r` |\n",
    "| `RETURN` | Specify what to output | `RETURN n.name, n.age` |\n",
    "| `WHERE` | Filter results | `WHERE n.age > 25` |\n",
    "| `ORDER BY` | Sort results | `ORDER BY n.name ASC` |\n",
    "| `LIMIT` | Restrict number of results | `LIMIT 10` |\n",
    "\n",
    "### üîß Essential Functions\n",
    "- `count()`: Count nodes/relationships\n",
    "- `collect()`: Aggregate values into lists\n",
    "- `size()`: Get collection size\n",
    "- `exists()`: Check if property/pattern exists\n",
    "- `toLower()`, `toUpper()`: String manipulation\n",
    "\n",
    "Let's start with basic queries to understand our current graph state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current graph state\n",
    "print(\"üîç Current Graph Overview:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Count all nodes\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (n) \n",
    "RETURN count(n) AS totalNodes\n",
    "\"\"\")\n",
    "print(f\"üìä Total nodes: {result[0]['totalNodes']}\")\n",
    "\n",
    "# Count nodes by label\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (n)\n",
    "RETURN labels(n)[0] AS nodeLabel, count(n) AS count\n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "print(\"\\nüìã Nodes by type:\")\n",
    "for row in result:\n",
    "    if row['nodeLabel']:  # Skip nodes without labels\n",
    "        print(f\"   {row['nodeLabel']}: {row['count']}\")\n",
    "\n",
    "# Count all relationships\n",
    "result = kg.query(\"\"\"\n",
    "MATCH ()-[r]->()\n",
    "RETURN count(r) AS totalRelationships\n",
    "\"\"\")\n",
    "print(f\"\\nüîó Total relationships: {result[0]['totalRelationships']}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "ixd5o09czy",
   "source": "# Safety check before proceeding with queries\\ndef check_kg_available():\\n    if 'kg' not in globals():\\n        print(\\\"‚ùå Neo4j connection not established!\\\")\\n        print(\\\"   Please run the connection health check above first.\\\")\\n        return False\\n    \\n    if not globals().get('connection_ok', False):\\n        print(\\\"‚ùå Neo4j connection health check failed!\\\")\\n        print(\\\"   Please fix connection issues before proceeding.\\\")\\n        return False\\n    \\n    return True\\n\\n# This will be used throughout the notebook to ensure connection is available\\nif check_kg_available():\\n    print(\\\"‚úÖ Neo4j connection verified - ready for queries!\\\")\\nelse:\\n    print(\\\"‚è∏Ô∏è Please establish connection first before running subsequent cells.\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "data-loading",
   "metadata": {},
   "source": [
    "## 5. Loading Data from CSV Files\n",
    "\n",
    "### üìÅ Data Structure\n",
    "We'll use structured CSV files to build our movie knowledge graph:\n",
    "- `movies.csv`: Movie nodes with properties\n",
    "- `people.csv`: Person nodes (actors, directors) \n",
    "- `genres.csv`: Genre nodes\n",
    "- `acted_in.csv`: Actor-Movie relationships\n",
    "- `directed.csv`: Director-Movie relationships  \n",
    "- `movie_genres.csv`: Movie-Genre relationships\n",
    "\n",
    "### üí° Loading Strategy\n",
    "1. **Clean existing data** (if needed)\n",
    "2. **Load nodes first** (movies, people, genres)\n",
    "3. **Create relationships** between existing nodes\n",
    "4. **Verify data integrity**\n",
    "\n",
    "Let's examine our data files first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "examine-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine our CSV data\n",
    "print(\"üìÇ Data File Overview:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "data_files = {\n",
    "    \"Movies\": \"data/movies.csv\",\n",
    "    \"People\": \"data/people.csv\", \n",
    "    \"Genres\": \"data/genres.csv\",\n",
    "    \"Acting Relationships\": \"data/acted_in.csv\",\n",
    "    \"Directing Relationships\": \"data/directed.csv\",\n",
    "    \"Movie-Genre Relationships\": \"data/movie_genres.csv\"\n",
    "}\n",
    "\n",
    "for name, file_path in data_files.items():\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"\\nüìä {name} ({file_path}):\")\n",
    "        print(f\"   Rows: {len(df)}, Columns: {list(df.columns)}\")\n",
    "        print(f\"   Sample: {df.iloc[0].to_dict() if len(df) > 0 else 'No data'}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\n‚ùå {name}: File not found - {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-data",
   "metadata": {},
   "source": [
    "### üßπ Clear Existing Data (Optional)\n",
    "Before loading new data, let's clean our database to start fresh:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CAUTION: This will delete all data in your graph!\n",
    "# Uncomment the lines below if you want to start with a clean database\n",
    "\n",
    "# print(\"‚ö†Ô∏è  Clearing all data from the database...\")\n",
    "# kg.query(\"MATCH (n) DETACH DELETE n\")\n",
    "# print(\"‚úÖ Database cleared successfully!\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Skipping database clear. Remove comments above to clear data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-nodes",
   "metadata": {},
   "source": [
    "### üì• Loading Node Data\n",
    "\n",
    "We'll use Cypher's `LOAD CSV` clause to efficiently import data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-movies",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Movie nodes\n",
    "print(\"üé¨ Loading Movies...\")\n",
    "\n",
    "cypher_movies = \"\"\"\n",
    "LOAD CSV WITH HEADERS FROM 'file:///data/movies.csv' AS row\n",
    "MERGE (m:Movie {title: row.title})\n",
    "SET m.released = toInteger(row.released),\n",
    "    m.tagline = row.tagline\n",
    "RETURN count(m) AS moviesCreated\n",
    "\"\"\"\n",
    "\n",
    "# Note: For local files, we need to use file:// protocol\n",
    "# Alternative approach using manual data loading:\n",
    "\n",
    "movies_df = pd.read_csv('data/movies.csv')\n",
    "movies_data = movies_df.to_dict('records')\n",
    "\n",
    "cypher_movies_manual = \"\"\"\n",
    "UNWIND $movies AS movie\n",
    "MERGE (m:Movie {title: movie.title})\n",
    "SET m.released = movie.released,\n",
    "    m.tagline = movie.tagline\n",
    "RETURN count(m) AS moviesCreated\n",
    "\"\"\"\n",
    "\n",
    "result = kg.query(cypher_movies_manual, params={\"movies\": movies_data})\n",
    "print(f\"‚úÖ Created {result[0]['moviesCreated']} movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-people",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load People nodes (actors and directors)\n",
    "print(\"üë• Loading People...\")\n",
    "\n",
    "people_df = pd.read_csv('data/people.csv')\n",
    "people_data = people_df.to_dict('records')\n",
    "\n",
    "cypher_people = \"\"\"\n",
    "UNWIND $people AS person\n",
    "MERGE (p:Person {name: person.name})\n",
    "SET p.born = person.born\n",
    "RETURN count(p) AS peopleCreated\n",
    "\"\"\"\n",
    "\n",
    "result = kg.query(cypher_people, params={\"people\": people_data})\n",
    "print(f\"‚úÖ Created {result[0]['peopleCreated']} people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-genres",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Genre nodes\n",
    "print(\"üé≠ Loading Genres...\")\n",
    "\n",
    "genres_df = pd.read_csv('data/genres.csv')\n",
    "genres_data = genres_df.to_dict('records')\n",
    "\n",
    "cypher_genres = \"\"\"\n",
    "UNWIND $genres AS genre\n",
    "MERGE (g:Genre {name: genre.name})\n",
    "RETURN count(g) AS genresCreated\n",
    "\"\"\"\n",
    "\n",
    "result = kg.query(cypher_genres, params={\"genres\": genres_data})\n",
    "print(f\"‚úÖ Created {result[0]['genresCreated']} genres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "load-relationships",
   "metadata": {},
   "source": [
    "### üîó Creating Relationships\n",
    "\n",
    "Now we'll connect our nodes with meaningful relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-acted-in",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ACTED_IN relationships\n",
    "print(\"üé≠ Creating ACTED_IN relationships...\")\n",
    "\n",
    "acted_in_df = pd.read_csv('data/acted_in.csv')\n",
    "acted_in_data = acted_in_df.to_dict('records')\n",
    "\n",
    "cypher_acted_in = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (a:Person {name: rel.actor})\n",
    "MATCH (m:Movie {title: rel.movie})\n",
    "MERGE (a)-[r:ACTED_IN]->(m)\n",
    "SET r.role = rel.role\n",
    "RETURN count(r) AS relationshipsCreated\n",
    "\"\"\"\n",
    "\n",
    "result = kg.query(cypher_acted_in, params={\"relationships\": acted_in_data})\n",
    "print(f\"‚úÖ Created {result[0]['relationshipsCreated']} ACTED_IN relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-directed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DIRECTED relationships\n",
    "print(\"üé¨ Creating DIRECTED relationships...\")\n",
    "\n",
    "directed_df = pd.read_csv('data/directed.csv')\n",
    "directed_data = directed_df.to_dict('records')\n",
    "\n",
    "cypher_directed = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (d:Person {name: rel.director})\n",
    "MATCH (m:Movie {title: rel.movie})\n",
    "MERGE (d)-[r:DIRECTED]->(m)\n",
    "RETURN count(r) AS relationshipsCreated\n",
    "\"\"\"\n",
    "\n",
    "result = kg.query(cypher_directed, params={\"relationships\": directed_data})\n",
    "print(f\"‚úÖ Created {result[0]['relationshipsCreated']} DIRECTED relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-movie-genres",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Movie-Genre relationships\n",
    "print(\"üè∑Ô∏è Creating BELONGS_TO relationships...\")\n",
    "\n",
    "movie_genres_df = pd.read_csv('data/movie_genres.csv')\n",
    "movie_genres_data = movie_genres_df.to_dict('records')\n",
    "\n",
    "cypher_movie_genres = \"\"\"\n",
    "UNWIND $relationships AS rel\n",
    "MATCH (m:Movie {title: rel.movie})\n",
    "MATCH (g:Genre {name: rel.genre})\n",
    "MERGE (m)-[r:BELONGS_TO]->(g)\n",
    "RETURN count(r) AS relationshipsCreated\n",
    "\"\"\"\n",
    "\n",
    "result = kg.query(cypher_movie_genres, params={\"relationships\": movie_genres_data})\n",
    "print(f\"‚úÖ Created {result[0]['relationshipsCreated']} BELONGS_TO relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verify-data",
   "metadata": {},
   "source": [
    "### ‚úÖ Data Verification\n",
    "\n",
    "Let's verify our data was loaded correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verify-loaded-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data verification\n",
    "print(\"üîç Data Verification Report\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Node counts by label\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (n)\n",
    "RETURN labels(n)[0] AS label, count(n) AS count\n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Node Summary:\")\n",
    "total_nodes = 0\n",
    "for row in result:\n",
    "    if row['label']:  # Skip unlabeled nodes\n",
    "        print(f\"   {row['label']}: {row['count']}\")\n",
    "        total_nodes += row['count']\n",
    "print(f\"   TOTAL: {total_nodes}\")\n",
    "\n",
    "# Relationship counts by type\n",
    "result = kg.query(\"\"\"\n",
    "MATCH ()-[r]->()\n",
    "RETURN type(r) AS relationshipType, count(r) AS count\n",
    "ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüîó Relationship Summary:\")\n",
    "total_relationships = 0\n",
    "for row in result:\n",
    "    print(f\"   {row['relationshipType']}: {row['count']}\")\n",
    "    total_relationships += row['count']\n",
    "print(f\"   TOTAL: {total_relationships}\")\n",
    "\n",
    "# Sample data verification\n",
    "print(\"\\nüîç Sample Data:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (p:Person)-[r:ACTED_IN]->(m:Movie)\n",
    "RETURN p.name AS actor, r.role AS role, m.title AS movie\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "\n",
    "for row in result:\n",
    "    print(f\"   {row['actor']} played {row['role']} in '{row['movie']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-cypher-queries",
   "metadata": {},
   "source": [
    "## 6. Basic Cypher Queries\n",
    "\n",
    "Now that we have data loaded, let's explore fundamental Cypher patterns:\n",
    "\n",
    "### üîç Pattern 1: Simple Node Matching\n",
    "The most basic operation is finding nodes by their properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-node-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Basic Node Queries\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Find all movies\n",
    "print(\"\\n1Ô∏è‚É£ All movies:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (m:Movie)\n",
    "RETURN m.title AS title, m.released AS year\n",
    "ORDER BY m.released DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "for movie in result:\n",
    "    print(f\"   üìΩÔ∏è {movie['title']} ({movie['year']})\")\n",
    "\n",
    "# 2. Find movies by specific criteria\n",
    "print(\"\\n2Ô∏è‚É£ Movies from the 1990s:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (m:Movie)\n",
    "WHERE m.released >= 1990 AND m.released < 2000\n",
    "RETURN m.title AS title, m.released AS year\n",
    "ORDER BY m.released\n",
    "\"\"\")\n",
    "for movie in result:\n",
    "    print(f\"   üìΩÔ∏è {movie['title']} ({movie['year']})\")\n",
    "\n",
    "# 3. Find a specific person\n",
    "print(\"\\n3Ô∏è‚É£ Information about Keanu Reeves:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (p:Person {name: \"Keanu Reeves\"})\n",
    "RETURN p.name AS name, p.born AS birthYear\n",
    "\"\"\")\n",
    "if result:\n",
    "    person = result[0]\n",
    "    print(f\"   üë§ {person['name']}, born {person['birthYear']}\")\n",
    "else:\n",
    "    print(\"   ‚ùå Person not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relationship-traversal",
   "metadata": {},
   "source": [
    "### üîó Pattern 2: Relationship Traversal\n",
    "The power of graphs lies in following relationships between entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relationship-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîó Relationship Traversal Queries\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. Find movies that Keanu Reeves acted in\n",
    "print(\"\\n1Ô∏è‚É£ Keanu Reeves' movies:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (p:Person {name: \"Keanu Reeves\"})-[r:ACTED_IN]->(m:Movie)\n",
    "RETURN m.title AS movie, r.role AS role, m.released AS year\n",
    "ORDER BY m.released\n",
    "\"\"\")\n",
    "for movie in result:\n",
    "    print(f\"   üé¨ {movie['movie']} ({movie['year']}) as {movie['role']}\")\n",
    "\n",
    "# 2. Find directors and their movies\n",
    "print(\"\\n2Ô∏è‚É£ Christopher Nolan's directed movies:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (d:Person {name: \"Christopher Nolan\"})-[:DIRECTED]->(m:Movie)\n",
    "RETURN m.title AS movie, m.released AS year\n",
    "ORDER BY m.released\n",
    "\"\"\")\n",
    "for movie in result:\n",
    "    print(f\"   üé≠ {movie['movie']} ({movie['year']})\")\n",
    "\n",
    "# 3. Find movies by genre\n",
    "print(\"\\n3Ô∏è‚É£ Sci-Fi movies:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (m:Movie)-[:BELONGS_TO]->(g:Genre {name: \"Sci-Fi\"})\n",
    "RETURN m.title AS movie, m.released AS year\n",
    "ORDER BY m.released DESC\n",
    "\"\"\")\n",
    "for movie in result:\n",
    "    print(f\"   üöÄ {movie['movie']} ({movie['year']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multi-hop-queries",
   "metadata": {},
   "source": [
    "### üåê Pattern 3: Multi-hop Relationships\n",
    "Graph databases excel at finding connections across multiple relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multi-hop-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üåê Multi-hop Relationship Queries\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. Find co-actors (actors who worked in the same movie)\n",
    "print(\"\\n1Ô∏è‚É£ Keanu Reeves' co-actors:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (keanu:Person {name: \"Keanu Reeves\"})-[:ACTED_IN]->(m:Movie)<-[:ACTED_IN]-(coactor:Person)\n",
    "WHERE keanu <> coactor  // Exclude Keanu himself\n",
    "RETURN coactor.name AS coactor, m.title AS movie\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    print(f\"   ü§ù {row['coactor']} in '{row['movie']}'\")\n",
    "\n",
    "# 2. Find actors who worked with specific directors\n",
    "print(\"\\n2Ô∏è‚É£ Actors who worked with Christopher Nolan:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (d:Person {name: \"Christopher Nolan\"})-[:DIRECTED]->(m:Movie)<-[:ACTED_IN]-(a:Person)\n",
    "RETURN DISTINCT a.name AS actor, count(m) AS movieCount, collect(m.title) AS movies\n",
    "ORDER BY movieCount DESC\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    movies_str = \", \".join(row['movies'][:3])  # Show first 3 movies\n",
    "    if len(row['movies']) > 3:\n",
    "        movies_str += \"...\"\n",
    "    print(f\"   üé≠ {row['actor']}: {row['movieCount']} movie(s) - {movies_str}\")\n",
    "\n",
    "# 3. Find movies that share multiple genres\n",
    "print(\"\\n3Ô∏è‚É£ Movies similar to 'The Matrix' (by genre):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (matrix:Movie {title: \"The Matrix\"})-[:BELONGS_TO]->(g:Genre)<-[:BELONGS_TO]-(similar:Movie)\n",
    "WHERE matrix <> similar\n",
    "WITH similar, count(g) AS sharedGenres, collect(g.name) AS genres\n",
    "RETURN similar.title AS movie, similar.released AS year, sharedGenres, genres\n",
    "ORDER BY sharedGenres DESC, similar.released DESC\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    genres_str = \", \".join(row['genres'])\n",
    "    print(f\"   üìΩÔ∏è {row['movie']} ({row['year']}) - {row['sharedGenres']} shared: {genres_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregation-analysis",
   "metadata": {},
   "source": [
    "## 7. Data Aggregation and Analysis\n",
    "\n",
    "### üìä Aggregation Functions\n",
    "Cypher provides powerful aggregation functions for data analysis:\n",
    "\n",
    "| Function | Purpose | Example |\n",
    "|----------|---------|--------|\n",
    "| `count()` | Count items | `count(n)`, `count(DISTINCT n.property)` |\n",
    "| `collect()` | Gather into list | `collect(n.name)` |\n",
    "| `sum()` | Sum numeric values | `sum(n.age)` |\n",
    "| `avg()` | Average values | `avg(n.rating)` |\n",
    "| `min()`, `max()` | Minimum/Maximum | `min(n.released)`, `max(n.released)` |\n",
    "| `size()` | Collection size | `size(collect(n))` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregation-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Data Aggregation and Analysis\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# 1. Most prolific actors (by number of movies)\n",
    "print(\"\\n1Ô∏è‚É£ Most prolific actors:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (a:Person)-[:ACTED_IN]->(m:Movie)\n",
    "WITH a, count(m) AS movieCount, collect(m.title) AS movies\n",
    "WHERE movieCount >= 1  // At least 1 movie\n",
    "RETURN a.name AS actor, movieCount, movies\n",
    "ORDER BY movieCount DESC, a.name\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    movies_preview = \", \".join(row['movies'][:2])  # Show first 2 movies\n",
    "    if len(row['movies']) > 2:\n",
    "        movies_preview += \"...\"\n",
    "    print(f\"   üèÜ {row['actor']}: {row['movieCount']} movies ({movies_preview})\")\n",
    "\n",
    "# 2. Most popular genres (by number of movies)\n",
    "print(\"\\n2Ô∏è‚É£ Most popular genres:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (g:Genre)<-[:BELONGS_TO]-(m:Movie)\n",
    "RETURN g.name AS genre, count(m) AS movieCount\n",
    "ORDER BY movieCount DESC\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    print(f\"   üìä {row['genre']}: {row['movieCount']} movies\")\n",
    "\n",
    "# 3. Movies by decade\n",
    "print(\"\\n3Ô∏è‚É£ Movies by decade:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (m:Movie)\n",
    "WITH (m.released / 10) * 10 AS decade, count(m) AS movieCount\n",
    "RETURN decade, movieCount\n",
    "ORDER BY decade DESC\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    print(f\"   üìÖ {int(row['decade'])}s: {row['movieCount']} movies\")\n",
    "\n",
    "# 4. Average career span analysis\n",
    "print(\"\\n4Ô∏è‚É£ Directors with multiple movies (career span):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (d:Person)-[:DIRECTED]->(m:Movie)\n",
    "WITH d, count(m) AS movieCount, \n",
    "     min(m.released) AS firstMovie, \n",
    "     max(m.released) AS lastMovie,\n",
    "     collect(m.title) AS movies\n",
    "WHERE movieCount > 1\n",
    "RETURN d.name AS director, movieCount, \n",
    "       firstMovie, lastMovie, \n",
    "       (lastMovie - firstMovie) AS careerSpan,\n",
    "       movies\n",
    "ORDER BY careerSpan DESC\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    movies_str = \", \".join(row['movies'])\n",
    "    print(f\"   üé¨ {row['director']}: {row['movieCount']} movies over {row['careerSpan']} years\")\n",
    "    print(f\"       ({row['firstMovie']}-{row['lastMovie']}) - {movies_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-patterns",
   "metadata": {},
   "source": [
    "## 8. Advanced Cypher Patterns\n",
    "\n",
    "### üß† Complex Query Patterns\n",
    "Let's explore more sophisticated graph analysis techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advanced-patterns",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß† Advanced Cypher Patterns\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Recommendation Engine: Find movies liked by people with similar tastes\n",
    "print(\"\\n1Ô∏è‚É£ Movie Recommendations for Sci-Fi fans:\")\n",
    "print(\"   (Movies that share genres with highly-rated Sci-Fi films)\")\n",
    "result = kg.query(\"\"\"\n",
    "// Find genres that appear in Sci-Fi movies\n",
    "MATCH (scifi:Genre {name: \"Sci-Fi\"})<-[:BELONGS_TO]-(scifiMovie:Movie)-[:BELONGS_TO]->(sharedGenre:Genre)\n",
    "WHERE sharedGenre.name <> \"Sci-Fi\"  // Exclude Sci-Fi itself\n",
    "\n",
    "// Find other movies with these shared genres\n",
    "MATCH (sharedGenre)<-[:BELONGS_TO]-(recommendedMovie:Movie)\n",
    "WHERE NOT (recommendedMovie)-[:BELONGS_TO]->(scifi)  // Exclude pure Sci-Fi movies\n",
    "\n",
    "// Count connections and return recommendations\n",
    "WITH recommendedMovie, count(DISTINCT sharedGenre) AS genreOverlap, collect(DISTINCT sharedGenre.name) AS sharedGenres\n",
    "RETURN recommendedMovie.title AS movie, recommendedMovie.released AS year, \n",
    "       genreOverlap, sharedGenres\n",
    "ORDER BY genreOverlap DESC, recommendedMovie.released DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    genres_str = \", \".join(row['sharedGenres'])\n",
    "    print(f\"   üí° {row['movie']} ({row['year']}) - {row['genreOverlap']} shared genres: {genres_str}\")\n",
    "\n",
    "# 2. Social Network Analysis: Find the \"Kevin Bacon\" of our graph\n",
    "print(\"\\n2Ô∏è‚É£ Most Connected Actor (Graph centrality):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (actor:Person)-[:ACTED_IN]->(m:Movie)<-[:ACTED_IN]-(coactor:Person)\n",
    "WHERE actor <> coactor\n",
    "WITH actor, count(DISTINCT coactor) AS connections, count(DISTINCT m) AS movies\n",
    "RETURN actor.name AS actor, connections, movies, \n",
    "       round(toFloat(connections) / movies, 2) AS avgCoactorsPerMovie\n",
    "ORDER BY connections DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    print(f\"   üåü {row['actor']}: {row['connections']} co-actors across {row['movies']} movies\")\n",
    "    print(f\"       Average {row['avgCoactorsPerMovie']} co-actors per movie\")\n",
    "\n",
    "# 3. Find \"Bridge\" Movies (connect different genres/communities)\n",
    "print(\"\\n3Ô∏è‚É£ Bridge Movies (connecting multiple genres):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (m:Movie)-[:BELONGS_TO]->(g:Genre)\n",
    "WITH m, count(g) AS genreCount, collect(g.name) AS genres\n",
    "WHERE genreCount >= 3  // Movies with 3+ genres\n",
    "RETURN m.title AS movie, m.released AS year, genreCount, genres\n",
    "ORDER BY genreCount DESC, m.released DESC\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    genres_str = \", \".join(row['genres'])\n",
    "    print(f\"   üåâ {row['movie']} ({row['year']}) - {row['genreCount']} genres: {genres_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-logic",
   "metadata": {},
   "source": [
    "### üîÄ Conditional Logic and Pattern Matching\n",
    "Advanced pattern matching with conditional logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-queries",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÄ Conditional Logic and Advanced Patterns\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. OPTIONAL MATCH - Handle missing relationships gracefully\n",
    "print(\"\\n1Ô∏è‚É£ People and their roles (actors vs directors):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (p:Person)\n",
    "OPTIONAL MATCH (p)-[acted:ACTED_IN]->(actedMovie:Movie)\n",
    "OPTIONAL MATCH (p)-[directed:DIRECTED]->(directedMovie:Movie)\n",
    "WITH p, \n",
    "     count(DISTINCT actedMovie) AS actedCount,\n",
    "     count(DISTINCT directedMovie) AS directedCount,\n",
    "     collect(DISTINCT actedMovie.title)[0..2] AS sampleActed,\n",
    "     collect(DISTINCT directedMovie.title)[0..2] AS sampleDirected\n",
    "WHERE actedCount > 0 OR directedCount > 0  // Has at least one relationship\n",
    "RETURN p.name AS person,\n",
    "       CASE \n",
    "         WHEN actedCount > 0 AND directedCount > 0 THEN \"Actor-Director\"\n",
    "         WHEN actedCount > 0 THEN \"Actor\"\n",
    "         WHEN directedCount > 0 THEN \"Director\"\n",
    "         ELSE \"Unknown\"\n",
    "       END AS role,\n",
    "       actedCount, directedCount, sampleActed, sampleDirected\n",
    "ORDER BY (actedCount + directedCount) DESC\n",
    "LIMIT 8\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    print(f\"   üë§ {row['person']} ({row['role']})\")\n",
    "    if row['actedCount'] > 0:\n",
    "        acted_sample = \", \".join([m for m in row['sampleActed'] if m])\n",
    "        print(f\"       Acted in {row['actedCount']} movies: {acted_sample}...\")\n",
    "    if row['directedCount'] > 0:\n",
    "        directed_sample = \", \".join([m for m in row['sampleDirected'] if m])\n",
    "        print(f\"       Directed {row['directedCount']} movies: {directed_sample}...\")\n",
    "\n",
    "# 2. Variable Length Paths - Find connections within N steps\n",
    "print(\"\\n2Ô∏è‚É£ Find connection paths between actors (within 3 steps):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH path = (start:Person {name: \"Keanu Reeves\"})-[*1..3]-(end:Person {name: \"Al Pacino\"})\n",
    "WHERE start <> end\n",
    "RETURN [node IN nodes(path) | \n",
    "        CASE \n",
    "          WHEN 'Person' IN labels(node) THEN node.name\n",
    "          WHEN 'Movie' IN labels(node) THEN node.title\n",
    "          ELSE 'Unknown'\n",
    "        END\n",
    "       ] AS connectionPath,\n",
    "       length(path) AS pathLength\n",
    "ORDER BY pathLength\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "print(\"   Connection paths from Keanu Reeves to Al Pacino:\")\n",
    "for i, row in enumerate(result):\n",
    "    path_str = \" ‚Üí \".join(row['connectionPath'])\n",
    "    print(f\"   {i+1}. {path_str} (length: {row['pathLength']})\")\n",
    "    \n",
    "if not result:\n",
    "    print(\"   ‚ùå No connection found within 3 steps\")\n",
    "\n",
    "# 3. Existence patterns - Movies without certain relationships\n",
    "print(\"\\n3Ô∏è‚É£ Movies missing genre information:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (m:Movie)\n",
    "WHERE NOT (m)-[:BELONGS_TO]->(:Genre)\n",
    "RETURN m.title AS movie, m.released AS year\n",
    "ORDER BY m.released DESC\n",
    "\"\"\")\n",
    "if result:\n",
    "    for row in result:\n",
    "        print(f\"   ‚ùì {row['movie']} ({row['year']}) - No genre assigned\")\n",
    "else:\n",
    "    print(\"   ‚úÖ All movies have genre information!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graph-algorithms",
   "metadata": {},
   "source": [
    "## 9. Graph Analysis and Insights\n",
    "\n",
    "### üìà Graph Metrics and Analysis\n",
    "Let's analyze our graph structure and discover insights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graph-analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìà Graph Structure Analysis\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. Graph density and connectivity\n",
    "print(\"\\n1Ô∏è‚É£ Graph Statistics:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (n)\n",
    "OPTIONAL MATCH (n)-[r]-()\n",
    "WITH labels(n)[0] AS nodeType, count(DISTINCT n) AS nodeCount, count(DISTINCT r) AS relationshipCount\n",
    "RETURN nodeType, nodeCount, relationshipCount\n",
    "ORDER BY nodeCount DESC\n",
    "\"\"\")\n",
    "total_nodes = sum([row['nodeCount'] for row in result if row['nodeType']])\n",
    "total_relationships = kg.query(\"MATCH ()-[r]->() RETURN count(r) AS total\")[0]['total']\n",
    "\n",
    "print(f\"   üìä Total nodes: {total_nodes}\")\n",
    "print(f\"   üîó Total relationships: {total_relationships}\")\n",
    "print(f\"   üìè Average degree: {round(total_relationships * 2 / total_nodes, 2)}\")\n",
    "\n",
    "# 2. Most influential nodes (highest degree centrality)\n",
    "print(\"\\n2Ô∏è‚É£ Most Connected Entities:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (n)-[r]-()\n",
    "WITH n, count(r) AS degree, labels(n)[0] AS nodeType\n",
    "RETURN nodeType, \n",
    "       CASE nodeType\n",
    "         WHEN 'Person' THEN n.name\n",
    "         WHEN 'Movie' THEN n.title\n",
    "         WHEN 'Genre' THEN n.name\n",
    "         ELSE 'Unknown'\n",
    "       END AS name,\n",
    "       degree\n",
    "ORDER BY degree DESC\n",
    "LIMIT 8\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    print(f\"   üåü {row['nodeType']}: {row['name']} (degree: {row['degree']})\")\n",
    "\n",
    "# 3. Community detection - Find tightly connected groups\n",
    "print(\"\\n3Ô∏è‚É£ Movie Clusters by Shared Actors:\")\n",
    "result = kg.query(\"\"\"\n",
    "// Find movies that share at least 2 actors\n",
    "MATCH (m1:Movie)<-[:ACTED_IN]-(a:Person)-[:ACTED_IN]->(m2:Movie)\n",
    "WHERE id(m1) < id(m2)  // Avoid duplicates\n",
    "WITH m1, m2, count(a) AS sharedActors\n",
    "WHERE sharedActors >= 2\n",
    "RETURN m1.title AS movie1, m2.title AS movie2, sharedActors\n",
    "ORDER BY sharedActors DESC\n",
    "\"\"\")\n",
    "if result:\n",
    "    for row in result:\n",
    "        print(f\"   üé≠ '{row['movie1']}' ‚Üî '{row['movie2']}': {row['sharedActors']} shared actors\")\n",
    "else:\n",
    "    print(\"   ‚ùå No movies share multiple actors\")\n",
    "\n",
    "# 4. Shortest path analysis\n",
    "print(\"\\n4Ô∏è‚É£ Six Degrees of Separation (Actor Network):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (a1:Person)-[:ACTED_IN]->(:Movie)<-[:ACTED_IN]-(a2:Person)\n",
    "WHERE a1 <> a2\n",
    "WITH a1, a2, 1 AS distance\n",
    "RETURN distance, count(*) AS pairs\n",
    "UNION ALL\n",
    "MATCH (a1:Person)-[:ACTED_IN]->(:Movie)<-[:ACTED_IN]-(:Person)-[:ACTED_IN]->(:Movie)<-[:ACTED_IN]-(a2:Person)\n",
    "WHERE a1 <> a2\n",
    "WITH a1, a2, 2 AS distance\n",
    "RETURN distance, count(*) AS pairs\n",
    "ORDER BY distance\n",
    "\"\"\")\n",
    "for row in result:\n",
    "    print(f\"   üîó {row['distance']} degree(s) of separation: {row['pairs']} actor pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "data-quality",
   "metadata": {},
   "source": [
    "### üîç Data Quality and Validation\n",
    "Let's check our data quality and find potential issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data-quality-checks",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Data Quality Analysis\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. Find orphaned nodes (nodes without relationships)\n",
    "print(\"\\n1Ô∏è‚É£ Orphaned Nodes (no relationships):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (n)\n",
    "WHERE NOT (n)-[]-() \n",
    "RETURN labels(n)[0] AS nodeType, count(n) AS orphanCount\n",
    "ORDER BY orphanCount DESC\n",
    "\"\"\")\n",
    "if result and any(row['orphanCount'] > 0 for row in result):\n",
    "    for row in result:\n",
    "        if row['orphanCount'] > 0:\n",
    "            print(f\"   ‚ö†Ô∏è {row['nodeType']}: {row['orphanCount']} orphaned nodes\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No orphaned nodes found!\")\n",
    "\n",
    "# 2. Find duplicate or similar names\n",
    "print(\"\\n2Ô∏è‚É£ Potential Data Quality Issues:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (p:Person)\n",
    "WITH p.name AS name, count(p) AS duplicateCount\n",
    "WHERE duplicateCount > 1\n",
    "RETURN name, duplicateCount\n",
    "\"\"\")\n",
    "if result:\n",
    "    print(\"   Duplicate person names:\")\n",
    "    for row in result:\n",
    "        print(f\"   ‚ö†Ô∏è '{row['name']}' appears {row['duplicateCount']} times\")\n",
    "else:\n",
    "    print(\"   ‚úÖ No duplicate person names\")\n",
    "\n",
    "# 3. Missing critical properties\n",
    "print(\"\\n3Ô∏è‚É£ Missing Properties:\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (m:Movie)\n",
    "WHERE m.released IS NULL OR m.tagline IS NULL\n",
    "RETURN m.title AS movie, \n",
    "       CASE WHEN m.released IS NULL THEN 'Missing release year' ELSE '' END +\n",
    "       CASE WHEN m.tagline IS NULL THEN 'Missing tagline' ELSE '' END AS issues\n",
    "\"\"\")\n",
    "if result:\n",
    "    for row in result:\n",
    "        if row['issues']:\n",
    "            print(f\"   ‚ö†Ô∏è {row['movie']}: {row['issues']}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ All movies have complete property data\")\n",
    "\n",
    "# 4. Relationship consistency check\n",
    "print(\"\\n4Ô∏è‚É£ Relationship Validation:\")\n",
    "result = kg.query(\"\"\"\n",
    "// Check for actors who also directed the same movie\n",
    "MATCH (p:Person)-[:ACTED_IN]->(m:Movie)<-[:DIRECTED]-(p)\n",
    "RETURN p.name AS person, m.title AS movie\n",
    "\"\"\")\n",
    "if result:\n",
    "    print(\"   Actor-Directors (acted in their own films):\")\n",
    "    for row in result:\n",
    "        print(f\"   üé≠üé¨ {row['person']} in '{row['movie']}'\")\n",
    "else:\n",
    "    print(\"   ‚ÑπÔ∏è No actor-directors found in the same movie\")\n",
    "\n",
    "# 5. Summary statistics\n",
    "print(\"\\n5Ô∏è‚É£ Data Completeness Summary:\")\n",
    "stats = {\n",
    "    \"Total Movies\": kg.query(\"MATCH (m:Movie) RETURN count(m) AS count\")[0]['count'],\n",
    "    \"Total People\": kg.query(\"MATCH (p:Person) RETURN count(p) AS count\")[0]['count'],\n",
    "    \"Total Genres\": kg.query(\"MATCH (g:Genre) RETURN count(g) AS count\")[0]['count'],\n",
    "    \"Acting Relationships\": kg.query(\"MATCH ()-[r:ACTED_IN]->() RETURN count(r) AS count\")[0]['count'],\n",
    "    \"Directing Relationships\": kg.query(\"MATCH ()-[r:DIRECTED]->() RETURN count(r) AS count\")[0]['count'],\n",
    "    \"Genre Relationships\": kg.query(\"MATCH ()-[r:BELONGS_TO]->() RETURN count(r) AS count\")[0]['count']\n",
    "}\n",
    "\n",
    "for metric, value in stats.items():\n",
    "    print(f\"   üìä {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-applications",
   "metadata": {},
   "source": [
    "## 10. Practical Applications and Use Cases\n",
    "\n",
    "### üéØ Real-World Scenarios\n",
    "Let's explore how these graph concepts apply to real business problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "practical-applications",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ Practical Applications\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. Content Recommendation System\n",
    "print(\"\\n1Ô∏è‚É£ Content Recommendation Engine:\")\n",
    "print(\"   Scenario: User likes 'The Matrix' - what should we recommend?\")\n",
    "\n",
    "result = kg.query(\"\"\"\n",
    "// Multi-factor recommendation based on:\n",
    "// 1. Shared genres\n",
    "// 2. Shared actors\n",
    "// 3. Similar release timeframe\n",
    "MATCH (liked:Movie {title: \"The Matrix\"})\n",
    "\n",
    "// Find movies with shared characteristics\n",
    "MATCH (candidate:Movie)\n",
    "WHERE candidate <> liked\n",
    "\n",
    "// Calculate genre similarity\n",
    "OPTIONAL MATCH (liked)-[:BELONGS_TO]->(sharedGenre:Genre)<-[:BELONGS_TO]-(candidate)\n",
    "WITH candidate, liked, count(DISTINCT sharedGenre) AS genreScore\n",
    "\n",
    "// Calculate actor similarity\n",
    "OPTIONAL MATCH (liked)<-[:ACTED_IN]-(sharedActor:Person)-[:ACTED_IN]->(candidate)\n",
    "WITH candidate, liked, genreScore, count(DISTINCT sharedActor) AS actorScore\n",
    "\n",
    "// Calculate time proximity (prefer movies within 5 years)\n",
    "WITH candidate, liked, genreScore, actorScore,\n",
    "     CASE WHEN abs(candidate.released - liked.released) <= 5 THEN 1 ELSE 0 END AS timeScore\n",
    "\n",
    "// Calculate overall recommendation score\n",
    "WITH candidate, (genreScore * 2 + actorScore * 3 + timeScore) AS recommendationScore,\n",
    "     genreScore, actorScore, timeScore\n",
    "WHERE recommendationScore > 0\n",
    "\n",
    "RETURN candidate.title AS movie, candidate.released AS year,\n",
    "       recommendationScore, genreScore, actorScore, timeScore\n",
    "ORDER BY recommendationScore DESC, candidate.released DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "for row in result:\n",
    "    print(f\"   üé¨ {row['movie']} ({row['year']}) - Score: {row['recommendationScore']}\")\n",
    "    print(f\"       Genre: {row['genreScore']}, Actor: {row['actorScore']}, Time: {row['timeScore']}\")\n",
    "\n",
    "# 2. Influencer Identification\n",
    "print(\"\\n2Ô∏è‚É£ Influencer Analysis (Key People in Network):\")\n",
    "result = kg.query(\"\"\"\n",
    "// Find people who connect different movie communities\n",
    "MATCH (p:Person)-[:ACTED_IN|DIRECTED]->(m:Movie)-[:BELONGS_TO]->(g:Genre)\n",
    "WITH p, count(DISTINCT m) AS movieCount, \n",
    "     count(DISTINCT g) AS genreSpread,\n",
    "     collect(DISTINCT g.name) AS genres\n",
    "WHERE movieCount >= 1 AND genreSpread >= 2\n",
    "RETURN p.name AS person, movieCount, genreSpread, genres,\n",
    "       round(toFloat(genreSpread) / movieCount, 2) AS diversityScore\n",
    "ORDER BY genreSpread DESC, movieCount DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "for row in result:\n",
    "    genres_str = \", \".join(row['genres'][:4])  # Show first 4 genres\n",
    "    print(f\"   üåü {row['person']}: {row['movieCount']} movies across {row['genreSpread']} genres\")\n",
    "    print(f\"       Diversity score: {row['diversityScore']} - Genres: {genres_str}\")\n",
    "\n",
    "# 3. Market Segmentation\n",
    "print(\"\\n3Ô∏è‚É£ Market Segmentation (Genre Preferences by Era):\")\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (m:Movie)-[:BELONGS_TO]->(g:Genre)\n",
    "WITH (m.released / 10) * 10 AS decade, g.name AS genre, count(m) AS movieCount\n",
    "WHERE decade >= 1990  // Focus on recent decades\n",
    "RETURN decade, genre, movieCount\n",
    "ORDER BY decade DESC, movieCount DESC\n",
    "\"\"\")\n",
    "\n",
    "# Group by decade for better presentation\n",
    "decades = {}\n",
    "for row in result:\n",
    "    decade = int(row['decade'])\n",
    "    if decade not in decades:\n",
    "        decades[decade] = []\n",
    "    decades[decade].append((row['genre'], row['movieCount']))\n",
    "\n",
    "for decade in sorted(decades.keys(), reverse=True):\n",
    "    print(f\"   üìÖ {decade}s - Top genres:\")\n",
    "    for genre, count in decades[decade][:3]:  # Top 3 per decade\n",
    "        print(f\"       {genre}: {count} movies\")\n",
    "\n",
    "# 4. Risk Assessment (Single Points of Failure)\n",
    "print(\"\\n4Ô∏è‚É£ Risk Assessment - Critical Dependencies:\")\n",
    "result = kg.query(\"\"\"\n",
    "// Find genres that would disappear if we lost certain movies\n",
    "MATCH (g:Genre)<-[:BELONGS_TO]-(m:Movie)\n",
    "WITH g, count(m) AS movieCount, collect(m.title) AS movies\n",
    "WHERE movieCount = 1  // Genres with only one movie\n",
    "RETURN g.name AS vulnerableGenre, movies[0] AS singleMovie\n",
    "\"\"\")\n",
    "\n",
    "if result:\n",
    "    print(\"   ‚ö†Ô∏è Vulnerable genres (dependent on single movies):\")\n",
    "    for row in result:\n",
    "        print(f\"       {row['vulnerableGenre']} ‚Üí only in '{row['singleMovie']}'\")\n",
    "else:\n",
    "    print(\"   ‚úÖ All genres are represented in multiple movies\")\n",
    "\n",
    "print(\"\\nüí° Business Insights Summary:\")\n",
    "print(\"   ‚Ä¢ Recommendation systems can leverage multiple relationship types\")\n",
    "print(\"   ‚Ä¢ Network analysis reveals influential people and content\")\n",
    "print(\"   ‚Ä¢ Trend analysis shows market evolution over time\")\n",
    "print(\"   ‚Ä¢ Risk assessment identifies critical dependencies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "performance-optimization",
   "metadata": {},
   "source": [
    "## 11. Performance Tips and Best Practices\n",
    "\n",
    "### ‚ö° Query Optimization\n",
    "\n",
    "#### üéØ Best Practices for Cypher Queries:\n",
    "\n",
    "1. **Use Labels and Indexes**\n",
    "   ```cypher\n",
    "   // Good: Use specific labels\n",
    "   MATCH (p:Person {name: \"Tom Hanks\"})\n",
    "   \n",
    "   // Avoid: Scanning all nodes\n",
    "   MATCH (p {name: \"Tom Hanks\"})\n",
    "   ```\n",
    "\n",
    "2. **Start with Most Selective Patterns**\n",
    "   ```cypher\n",
    "   // Good: Start with specific node\n",
    "   MATCH (specific:Movie {title: \"The Matrix\"})-[:BELONGS_TO]->(g:Genre)\n",
    "   \n",
    "   // Avoid: Start with broad pattern\n",
    "   MATCH (g:Genre)<-[:BELONGS_TO]-(m:Movie {title: \"The Matrix\"})\n",
    "   ```\n",
    "\n",
    "3. **Use LIMIT Early**\n",
    "   ```cypher\n",
    "   // Good: Limit early in processing\n",
    "   MATCH (p:Person)-[:ACTED_IN]->(m:Movie)\n",
    "   WITH p, count(m) AS movieCount\n",
    "   ORDER BY movieCount DESC\n",
    "   LIMIT 10\n",
    "   RETURN p.name, movieCount\n",
    "   ```\n",
    "\n",
    "#### üìä Index Creation\n",
    "For production systems, create indexes on frequently queried properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "performance-tips",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚ö° Performance Tips and Index Management\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Check existing indexes\n",
    "print(\"\\n1Ô∏è‚É£ Current Database Indexes:\")\n",
    "try:\n",
    "    result = kg.query(\"SHOW INDEXES\")\n",
    "    if result:\n",
    "        for idx in result:\n",
    "            print(f\"   üìá {idx.get('name', 'unnamed')}: {idx.get('labelsOrTypes', [])} - {idx.get('properties', [])}\")\n",
    "    else:\n",
    "        print(\"   ‚ÑπÔ∏è No indexes found or command not supported\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ö†Ô∏è Could not retrieve indexes: {str(e)[:100]}...\")\n",
    "\n",
    "# 2. Query performance demonstration\n",
    "print(\"\\n2Ô∏è‚É£ Query Performance Examples:\")\n",
    "\n",
    "# Example of efficient vs inefficient queries\n",
    "import time\n",
    "\n",
    "# Efficient query - using labels and specific properties\n",
    "start_time = time.time()\n",
    "result = kg.query(\"\"\"\n",
    "MATCH (p:Person {name: \"Keanu Reeves\"})-[:ACTED_IN]->(m:Movie)\n",
    "RETURN count(m) AS movieCount\n",
    "\"\"\")\n",
    "efficient_time = time.time() - start_time\n",
    "\n",
    "# Less efficient query - broader pattern\n",
    "start_time = time.time()\n",
    "result2 = kg.query(\"\"\"\n",
    "MATCH (p:Person)-[:ACTED_IN]->(m:Movie)\n",
    "WHERE p.name = \"Keanu Reeves\"\n",
    "RETURN count(m) AS movieCount\n",
    "\"\"\")\n",
    "less_efficient_time = time.time() - start_time\n",
    "\n",
    "print(f\"   üèÉ‚Äç‚ôÇÔ∏è Efficient query (with node property): {efficient_time:.4f} seconds\")\n",
    "print(f\"   üö∂‚Äç‚ôÇÔ∏è Less efficient query (with WHERE): {less_efficient_time:.4f} seconds\")\n",
    "\n",
    "# 3. Memory-conscious queries\n",
    "print(\"\\n3Ô∏è‚É£ Memory-Efficient Patterns:\")\n",
    "print(\"   üí° Use LIMIT and pagination for large result sets\")\n",
    "print(\"   üí° Avoid collecting large lists - use aggregation instead\")\n",
    "print(\"   üí° Use EXPLAIN/PROFILE to analyze query execution\")\n",
    "\n",
    "# Example of memory-conscious aggregation\n",
    "result = kg.query(\"\"\"\n",
    "// Memory-efficient: Count instead of collecting\n",
    "MATCH (g:Genre)<-[:BELONGS_TO]-(m:Movie)\n",
    "RETURN g.name AS genre, count(m) AS movieCount\n",
    "ORDER BY movieCount DESC\n",
    "LIMIT 3\n",
    "\"\"\")\n",
    "print(\"\\n   üìä Top genres (memory-efficient counting):\")\n",
    "for row in result:\n",
    "    print(f\"       {row['genre']}: {row['movieCount']} movies\")\n",
    "\n",
    "# 4. Best practices summary\n",
    "print(\"\\n4Ô∏è‚É£ Query Optimization Checklist:\")\n",
    "best_practices = [\n",
    "    \"‚úÖ Always use node labels in MATCH clauses\",\n",
    "    \"‚úÖ Start patterns with most selective nodes\", \n",
    "    \"‚úÖ Use indexes on frequently queried properties\",\n",
    "    \"‚úÖ Apply LIMIT early in query processing\",\n",
    "    \"‚úÖ Use DISTINCT only when necessary\",\n",
    "    \"‚úÖ Prefer aggregation over large collections\",\n",
    "    \"‚úÖ Use OPTIONAL MATCH for optional relationships\",\n",
    "    \"‚úÖ Profile queries with EXPLAIN/PROFILE\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f\"   {practice}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Next Steps\n",
    "\n",
    "### üéâ What You've Learned\n",
    "\n",
    "Congratulations! You've completed a comprehensive tour of Knowledge Graphs and Cypher. Here's what you've mastered:\n",
    "\n",
    "#### üìö **Conceptual Understanding**\n",
    "- ‚úÖ Knowledge Graph fundamentals and use cases\n",
    "- ‚úÖ Graph vs. relational database concepts\n",
    "- ‚úÖ Neo4j architecture and labeled property graphs\n",
    "\n",
    "#### üîß **Technical Skills**\n",
    "- ‚úÖ Cypher query language syntax and patterns\n",
    "- ‚úÖ Data loading from CSV files\n",
    "- ‚úÖ Basic to advanced graph traversals\n",
    "- ‚úÖ Aggregation and analytical queries\n",
    "- ‚úÖ Performance optimization techniques\n",
    "\n",
    "#### üéØ **Practical Applications**\n",
    "- ‚úÖ Recommendation systems\n",
    "- ‚úÖ Social network analysis\n",
    "- ‚úÖ Data quality assessment\n",
    "- ‚úÖ Business intelligence and insights\n",
    "\n",
    "### üöÄ **Next Steps**\n",
    "\n",
    "#### **Immediate Actions:**\n",
    "1. **Practice**: Try modifying the queries in this notebook\n",
    "2. **Experiment**: Load your own CSV data into the graph\n",
    "3. **Explore**: Try the Neo4j Browser interface for visual exploration\n",
    "\n",
    "#### **Advanced Topics to Explore:**\n",
    "- **Graph Data Science Library (GDS)**: Advanced algorithms for centrality, community detection, similarity\n",
    "- **APOC Procedures**: Extended functionality for data processing and analysis\n",
    "- **Graph Neural Networks**: Machine learning on graph data\n",
    "- **Multi-database setups**: Sharding and federation\n",
    "- **Real-time graph streaming**: Kafka integration and live updates\n",
    "\n",
    "#### **Resources for Continued Learning:**\n",
    "- üìñ [Neo4j Documentation](https://neo4j.com/docs/)\n",
    "- üéì [Neo4j GraphAcademy](https://graphacademy.neo4j.com/)\n",
    "- üë• [Neo4j Community Forum](https://community.neo4j.com/)\n",
    "- üìä [Graph Data Science Playground](https://neo4j.com/sandbox/)\n",
    "\n",
    "### üí° **Final Tips**\n",
    "- **Think in Relationships**: Always consider how your entities connect\n",
    "- **Start Simple**: Begin with basic patterns and build complexity gradually\n",
    "- **Visualize**: Use Neo4j Browser or Bloom to explore your graphs visually\n",
    "- **Performance Matters**: Always consider indexing and query optimization\n",
    "- **Community**: Join the Neo4j community - it's incredibly helpful and welcoming!\n",
    "\n",
    "---\n",
    "\n",
    "### üôè **Thank You!**\n",
    "You've completed the Knowledge Graphs and Cypher tutorial. You now have the foundation to build powerful graph-based applications and analyses. The world of connected data awaits your exploration!\n",
    "\n",
    "**Happy Graphing!** üéä"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fj1iyvcjhbv",
   "source": "## 14. GraphRAG Integration - Financial Investment Advisor\\n\\n### üè¶ Building a Real-World Financial Analysis System\\n\\nGraphRAG (Graph Retrieval-Augmented Generation) combines the power of Knowledge Graphs with Large Language Models to provide contextual, accurate responses based on structured data relationships.\\n\\n#### üí° **What We'll Build:**\\nA **Financial Investment Advisor** that:\\n- Ingests SEC EDGAR 10-K annual reports from major companies\\n- Extracts financial entities, risks, and key metrics\\n- Creates a knowledge graph of financial relationships\\n- Uses vector embeddings for semantic search\\n- Provides AI-powered investment recommendations\\n\\n#### üéØ **Business Impact:**\\n- **Investment Decision Support**: Data-driven investment recommendations\\n- **Risk Analysis**: Identify and assess financial risks across portfolios\\n- **Market Intelligence**: Understand company relationships and dependencies\\n- **Compliance**: Track regulatory changes and their business impacts\\n\\n#### üõ†Ô∏è **Technology Stack (All Open Source):**\\n- **Data Source**: SEC EDGAR API (free, public financial data)\\n- **LLM**: Ollama (Llama 3.1) or HuggingFace Transformers\\n- **Embeddings**: sentence-transformers (all-MiniLM-L6-v2)\\n- **Graph Database**: Neo4j Aura Free Tier\\n- **Vector Search**: Neo4j built-in vector indexing\\n\\n---\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "hv6u9x48t2t",
   "source": "### üöÄ Setup and Dependencies\\n\\nFirst, let's install and import the additional libraries needed for our financial GraphRAG system:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "b4gqi5oyxa4",
   "source": "# Install additional dependencies (run once)\\n# !pip install sentence-transformers transformers torch requests beautifulsoup4 lxml yfinance plotly wordcloud\\n\\n# Import libraries for GraphRAG Financial Advisor\\nimport requests\\nimport json\\nfrom bs4 import BeautifulSoup\\nimport re\\nfrom datetime import datetime, timedelta\\nfrom typing import List, Dict, Any, Optional, Tuple\\nfrom dataclasses import dataclass\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n# ML and NLP libraries\\ntry:\\n    from sentence_transformers import SentenceTransformer\\n    from transformers import pipeline, AutoTokenizer, AutoModel\\n    import torch\\n    print(\\\"‚úÖ ML libraries loaded successfully\\\")\\nexcept ImportError as e:\\n    print(f\\\"‚ö†Ô∏è ML libraries not available: {e}\\\")\\n    print(\\\"Please run: pip install sentence-transformers transformers torch\\\")\\n\\n# Data processing\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.metrics.pairwise import cosine_similarity\\n\\n# Visualization (optional)\\ntry:\\n    import plotly.graph_objects as go\\n    import plotly.express as px\\n    from wordcloud import WordCloud\\n    import matplotlib.pyplot as plt\\n    print(\\\"‚úÖ Visualization libraries loaded\\\")\\nexcept ImportError:\\n    print(\\\"‚ÑπÔ∏è Visualization libraries not available (optional)\\\")\\n\\nprint(\\\"üîß GraphRAG Financial Advisor dependencies loaded!\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "h6jlvuynoq",
   "source": "### üìä SEC EDGAR Data Extraction Pipeline\\n\\nThe SEC EDGAR database contains comprehensive financial information about public companies. We'll create a pipeline to extract and process 10-K annual reports.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "mpcfd0mgppr",
   "source": "# SEC EDGAR Data Extractor\\n@dataclass\\nclass CompanyInfo:\\n    \\\"\\\"\\\"Structure for company information\\\"\\\"\\\"\\n    cik: str\\n    ticker: str\\n    name: str\\n    sector: str = \\\"\\\"\\n    industry: str = \\\"\\\"\\n\\n@dataclass \\nclass FinancialDocument:\\n    \\\"\\\"\\\"Structure for financial documents\\\"\\\"\\\"\\n    company: CompanyInfo\\n    document_type: str\\n    filing_date: str\\n    period_end: str\\n    content: Dict[str, str]  # section_name: content\\n    url: str\\n\\nclass SECDataExtractor:\\n    \\\"\\\"\\\"Extract and process SEC EDGAR financial data\\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.base_url = \\\"https://data.sec.gov\\\"\\n        self.headers = {\\n            'User-Agent': 'Neo4j GraphRAG Tutorial (educational@example.com)',\\n            'Accept-Encoding': 'gzip, deflate',\\n            'Host': 'data.sec.gov'\\n        }\\n        \\n        # Major companies for demonstration (CIK numbers)\\n        self.demo_companies = {\\n            'AAPL': CompanyInfo('320193', 'AAPL', 'Apple Inc.', 'Technology', 'Consumer Electronics'),\\n            'MSFT': CompanyInfo('789019', 'MSFT', 'Microsoft Corporation', 'Technology', 'Software'),\\n            'GOOGL': CompanyInfo('1652044', 'GOOGL', 'Alphabet Inc.', 'Technology', 'Internet Services'),\\n            'TSLA': CompanyInfo('1318605', 'TSLA', 'Tesla, Inc.', 'Automotive', 'Electric Vehicles'),\\n            'AMZN': CompanyInfo('1018724', 'AMZN', 'Amazon.com, Inc.', 'Technology', 'E-commerce')\\n        }\\n    \\n    def get_company_filings(self, cik: str, form_type: str = '10-K', limit: int = 3) -> List[Dict]:\\n        \\\"\\\"\\\"Get recent filings for a company\\\"\\\"\\\"\\n        url = f\\\"{self.base_url}/submissions/CIK{cik.zfill(10)}.json\\\"\\n        \\n        try:\\n            response = requests.get(url, headers=self.headers)\\n            response.raise_for_status()\\n            data = response.json()\\n            \\n            # Extract recent filings\\n            filings = []\\n            recent_filings = data.get('filings', {}).get('recent', {})\\n            \\n            forms = recent_filings.get('form', [])\\n            dates = recent_filings.get('filingDate', [])\\n            accession_numbers = recent_filings.get('accessionNumber', [])\\n            \\n            for i, form in enumerate(forms):\\n                if form == form_type and len(filings) < limit:\\n                    filings.append({\\n                        'form': form,\\n                        'filing_date': dates[i],\\n                        'accession_number': accession_numbers[i]\\n                    })\\n            \\n            return filings\\n            \\n        except requests.RequestException as e:\\n            print(f\\\"‚ùå Error fetching filings for CIK {cik}: {e}\\\")\\n            return []\\n    \\n    def extract_10k_sections(self, content: str) -> Dict[str, str]:\\n        \\\"\\\"\\\"Extract key sections from 10-K filing content\\\"\\\"\\\"\\n        sections = {}\\n        \\n        # Common 10-K sections to extract\\n        section_patterns = {\\n            'business_overview': r'item\\\\s*1[\\\\s\\\\.].*?business',\\n            'risk_factors': r'item\\\\s*1a[\\\\s\\\\.].*?risk\\\\s*factors',\\n            'properties': r'item\\\\s*2[\\\\s\\\\.].*?properties',\\n            'legal_proceedings': r'item\\\\s*3[\\\\s\\\\.].*?legal\\\\s*proceedings',\\n            'management_discussion': r'item\\\\s*7[\\\\s\\\\.].*?management.*?discussion',\\n            'financial_statements': r'item\\\\s*8[\\\\s\\\\.].*?financial\\\\s*statements',\\n            'controls_procedures': r'item\\\\s*9a[\\\\s\\\\.].*?controls.*?procedures'\\n        }\\n        \\n        content_lower = content.lower()\\n        \\n        for section_name, pattern in section_patterns.items():\\n            try:\\n                # Find section start\\n                match = re.search(pattern, content_lower, re.IGNORECASE | re.DOTALL)\\n                if match:\\n                    start_pos = match.start()\\n                    \\n                    # Find next section or end (simplified extraction)\\n                    # In production, you'd want more sophisticated parsing\\n                    end_pos = start_pos + 5000  # Take first 5000 chars as sample\\n                    section_text = content[start_pos:min(end_pos, len(content))]\\n                    \\n                    # Clean up the text\\n                    section_text = re.sub(r'\\\\s+', ' ', section_text)  # Normalize whitespace\\n                    section_text = section_text.strip()\\n                    \\n                    if len(section_text) > 100:  # Only include substantial content\\n                        sections[section_name] = section_text\\n                        \\n            except Exception as e:\\n                print(f\\\"‚ö†Ô∏è Error extracting {section_name}: {e}\\\")\\n        \\n        return sections\\n    \\n    def get_demo_financial_data(self) -> List[FinancialDocument]:\\n        \\\"\\\"\\\"Get sample financial data for demonstration\\\"\\\"\\\"\\n        print(\\\"üìä Extracting sample financial data for major companies...\\\")\\n        \\n        documents = []\\n        \\n        # For demo purposes, we'll create sample documents with realistic content\\n        # In production, you'd fetch real SEC data\\n        sample_data = {\\n            'AAPL': {\\n                'business_overview': \\\"\\\"\\\"\\n                Apple Inc. designs, manufactures and markets smartphones, personal computers, tablets, \\n                wearables and accessories worldwide. The Company's products include iPhone, Mac, iPad, \\n                AirPods, Apple TV, Apple Watch, Beats products, and HomePod. The Company also sells \\n                various related services including advertising, AppleCare, cloud services, digital content \\n                stores and streaming, and payment services.\\n                \\\"\\\"\\\",\\n                'risk_factors': \\\"\\\"\\\"\\n                The Company is subject to intense competition in all areas of its business, and it competes \\n                on price, product features, relative price/performance, product quality and reliability, \\n                design innovation, a strong third-party software and accessories ecosystem, marketing and \\n                distribution capability, service and support, and corporate reputation. Global markets for \\n                the Company's products and services are highly competitive and subject to rapid technological \\n                change, and the Company may be unable to compete effectively in these markets.\\n                \\\"\\\"\\\",\\n                'management_discussion': \\\"\\\"\\\"\\n                Net sales increased during 2023 compared to 2022 due to higher net sales of iPhone, Services \\n                and Mac, partially offset by lower net sales of iPad and Wearables, Home and Accessories. \\n                iPhone net sales increased due to higher sales volume of iPhone 15 models. Services net sales \\n                growth was driven by advertising, AppleCare and the App Store.\\n                \\\"\\\"\\\"\\n            },\\n            'MSFT': {\\n                'business_overview': \\\"\\\"\\\"\\n                Microsoft Corporation develops, licenses, and supports software, services, devices and solutions \\n                worldwide. The company operates through three segments: Productivity and Business Processes, \\n                Intelligent Cloud, and More Personal Computing. We strive to create local opportunity, growth, \\n                and impact in every country around the world.\\n                \\\"\\\"\\\",\\n                'risk_factors': \\\"\\\"\\\"\\n                Security threats and cyberattacks could lead to reduced revenue, increased costs, liability \\n                claims, or harm to our reputation or competitive position. Cyberattacks are a critical risk \\n                for our company and our customers. We devote substantial resources to defend against security \\n                threats, but these measures may be insufficient.\\n                \\\"\\\"\\\",\\n                'management_discussion': \\\"\\\"\\\"\\n                Revenue increased $28.9 billion or 16% driven by growth across our three segments. Productivity \\n                and Business Processes revenue increased driven by Microsoft 365 Commercial and Microsoft Teams. \\n                Intelligent Cloud revenue increased primarily due to growth in Azure and other cloud services.\\n                \\\"\\\"\\\"\\n            }\\n        }\\n        \\n        for ticker, company in self.demo_companies.items():\\n            if ticker in sample_data:\\n                doc = FinancialDocument(\\n                    company=company,\\n                    document_type='10-K',\\n                    filing_date='2023-10-27',\\n                    period_end='2023-09-30',\\n                    content=sample_data[ticker],\\n                    url=f\\\"https://sec.gov/demo/{ticker}/10-K/2023\\\"\\n                )\\n                documents.append(doc)\\n        \\n        print(f\\\"‚úÖ Extracted {len(documents)} sample financial documents\\\")\\n        return documents\\n\\n# Initialize the SEC data extractor\\nsec_extractor = SECDataExtractor()\\n\\n# Get demo financial data\\nfinancial_documents = sec_extractor.get_demo_financial_data()\\n\\nprint(f\\\"\\\\nüìã Available Companies:\\\")\\nfor ticker, company in sec_extractor.demo_companies.items():\\n    print(f\\\"   {ticker}: {company.name} ({company.sector})\\\")\\n\\nprint(f\\\"\\\\nüìÑ Sample Document Content:\\\")\\nif financial_documents:\\n    sample_doc = financial_documents[0]\\n    print(f\\\"Company: {sample_doc.company.name}\\\")\\n    print(f\\\"Sections: {list(sample_doc.content.keys())}\\\")\\n    print(f\\\"Business Overview (first 200 chars): {sample_doc.content.get('business_overview', '')[:200]}...\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "mnlmsnppwqn",
   "source": "### ü§ñ Open Source LLM Integration\\n\\nWe'll integrate open-source language models for text processing and generation. This setup supports both local Ollama models and HuggingFace models.\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hr3nwnw8o6b",
   "source": "# Open Source LLM Integration\\nclass LLMManager:\\n    \\\"\\\"\\\"Manager for different LLM backends (Ollama, HuggingFace, etc.)\\\"\\\"\\\"\\n    \\n    def __init__(self):\\n        self.embedding_model = None\\n        self.text_generator = None\\n        self.available_backends = self._detect_backends()\\n        self.setup_models()\\n    \\n    def _detect_backends(self) -> Dict[str, bool]:\\n        \\\"\\\"\\\"Detect which LLM backends are available\\\"\\\"\\\"\\n        backends = {\\n            'sentence_transformers': False,\\n            'transformers': False,\\n            'ollama': False\\n        }\\n        \\n        try:\\n            import sentence_transformers\\n            backends['sentence_transformers'] = True\\n            print(\\\"‚úÖ sentence-transformers available\\\")\\n        except ImportError:\\n            print(\\\"‚ùå sentence-transformers not available\\\")\\n        \\n        try:\\n            import transformers\\n            backends['transformers'] = True\\n            print(\\\"‚úÖ transformers available\\\")\\n        except ImportError:\\n            print(\\\"‚ùå transformers not available\\\")\\n        \\n        try:\\n            import ollama\\n            # Try to connect to Ollama\\n            response = ollama.list()\\n            backends['ollama'] = True\\n            print(\\\"‚úÖ Ollama available\\\")\\n        except:\\n            print(\\\"‚ùå Ollama not available (install Ollama and pull a model)\\\")\\n        \\n        return backends\\n    \\n    def setup_models(self):\\n        \\\"\\\"\\\"Setup embedding and text generation models\\\"\\\"\\\"\\n        print(\\\"\\\\nüîß Setting up models...\\\")\\n        \\n        # Setup embedding model\\n        if self.available_backends['sentence_transformers']:\\n            try:\\n                # Use a lightweight, fast model for embeddings\\n                self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\\n                print(\\\"‚úÖ Embedding model loaded: all-MiniLM-L6-v2\\\")\\n            except Exception as e:\\n                print(f\\\"‚ö†Ô∏è Could not load embedding model: {e}\\\")\\n        \\n        # Setup text generation model\\n        self._setup_text_generator()\\n    \\n    def _setup_text_generator(self):\\n        \\\"\\\"\\\"Setup text generation model with fallback options\\\"\\\"\\\"\\n        \\n        # Option 1: Try Ollama (best for local inference)\\n        if self.available_backends['ollama']:\\n            try:\\n                import ollama\\n                # Check if we have a suitable model\\n                models = ollama.list().get('models', [])\\n                suitable_models = ['llama3.1:8b', 'llama3.1', 'mistral', 'phi3']\\n                \\n                available_model = None\\n                for model in suitable_models:\\n                    if any(m['name'].startswith(model) for m in models):\\n                        available_model = model\\n                        break\\n                \\n                if available_model:\\n                    self.text_generator = OllamaGenerator(available_model)\\n                    print(f\\\"‚úÖ Using Ollama model: {available_model}\\\")\\n                    return\\n                else:\\n                    print(\\\"‚ö†Ô∏è No suitable Ollama models found\\\")\\n                    print(\\\"üí° Install Ollama and run: ollama pull llama3.1:8b\\\")\\n            except Exception as e:\\n                print(f\\\"‚ö†Ô∏è Ollama setup failed: {e}\\\")\\n        \\n        # Option 2: HuggingFace Transformers (fallback)\\n        if self.available_backends['transformers']:\\n            try:\\n                # Use a lightweight model for demonstration\\n                self.text_generator = HuggingFaceGenerator('microsoft/DialoGPT-medium')\\n                print(\\\"‚úÖ Using HuggingFace model: microsoft/DialoGPT-medium\\\")\\n                return\\n            except Exception as e:\\n                print(f\\\"‚ö†Ô∏è HuggingFace setup failed: {e}\\\")\\n        \\n        # Option 3: Simple rule-based fallback\\n        self.text_generator = SimpleGenerator()\\n        print(\\\"‚ÑπÔ∏è Using simple rule-based text generator (fallback)\\\")\\n    \\n    def get_embeddings(self, texts: List[str]) -> np.ndarray:\\n        \\\"\\\"\\\"Get embeddings for a list of texts\\\"\\\"\\\"\\n        if self.embedding_model is None:\\n            # Fallback to simple TF-IDF if no model available\\n            from sklearn.feature_extraction.text import TfidfVectorizer\\n            vectorizer = TfidfVectorizer(max_features=384)  # Match embedding dimension\\n            return vectorizer.fit_transform(texts).toarray()\\n        \\n        return self.embedding_model.encode(texts)\\n    \\n    def generate_text(self, prompt: str, max_length: int = 200) -> str:\\n        \\\"\\\"\\\"Generate text using the available model\\\"\\\"\\\"\\n        if self.text_generator is None:\\n            return \\\"Text generation not available - please setup a model\\\"\\n        \\n        return self.text_generator.generate(prompt, max_length)\\n\\n# Different text generator implementations\\nclass OllamaGenerator:\\n    \\\"\\\"\\\"Ollama-based text generator\\\"\\\"\\\"\\n    \\n    def __init__(self, model_name: str):\\n        import ollama\\n        self.client = ollama\\n        self.model_name = model_name\\n    \\n    def generate(self, prompt: str, max_length: int = 200) -> str:\\n        try:\\n            response = self.client.generate(\\n                model=self.model_name,\\n                prompt=prompt,\\n                options={'num_predict': max_length}\\n            )\\n            return response.get('response', 'No response generated')\\n        except Exception as e:\\n            return f\\\"Error generating text: {e}\\\"\\n\\nclass HuggingFaceGenerator:\\n    \\\"\\\"\\\"HuggingFace transformers-based generator\\\"\\\"\\\"\\n    \\n    def __init__(self, model_name: str):\\n        self.generator = pipeline('text-generation', model=model_name)\\n    \\n    def generate(self, prompt: str, max_length: int = 200) -> str:\\n        try:\\n            outputs = self.generator(\\n                prompt, \\n                max_length=max_length, \\n                num_return_sequences=1,\\n                temperature=0.7\\n            )\\n            return outputs[0]['generated_text'].replace(prompt, '').strip()\\n        except Exception as e:\\n            return f\\\"Error generating text: {e}\\\"\\n\\nclass SimpleGenerator:\\n    \\\"\\\"\\\"Simple rule-based generator for fallback\\\"\\\"\\\"\\n    \\n    def generate(self, prompt: str, max_length: int = 200) -> str:\\n        # Simple template-based responses for financial analysis\\n        if 'investment' in prompt.lower():\\n            return \\\"Based on the available data, consider diversifying your portfolio and consulting with a financial advisor.\\\"\\n        elif 'risk' in prompt.lower():\\n            return \\\"Key risks include market volatility, regulatory changes, and competitive pressures. Monitor these factors closely.\\\"\\n        elif 'recommend' in prompt.lower():\\n            return \\\"Recommendations should be based on thorough analysis of financial statements, market conditions, and your investment goals.\\\"\\n        else:\\n            return \\\"Please provide more specific financial questions for detailed analysis.\\\"\\n\\n# Initialize the LLM manager\\nprint(\\\"üöÄ Initializing LLM Manager...\\\")\\nllm_manager = LLMManager()\\n\\n# Test the setup\\nprint(\\\"\\\\nüß™ Testing LLM Setup:\\\")\\nif llm_manager.embedding_model:\\n    test_texts = [\\\"Apple Inc. financial performance\\\", \\\"Technology sector risks\\\"]\\n    embeddings = llm_manager.get_embeddings(test_texts)\\n    print(f\\\"‚úÖ Embeddings shape: {embeddings.shape}\\\")\\nelse:\\n    print(\\\"‚ö†Ô∏è No embedding model available\\\")\\n\\n# Test text generation\\ntest_prompt = \\\"What are the key factors to consider when analyzing a technology company's financial health?\\\"\\nresponse = llm_manager.generate_text(test_prompt, max_length=100)\\nprint(f\\\"\\\\nü§ñ Sample AI Response:\\\\n{response[:200]}...\\\")\\n\\nprint(\\\"\\\\n‚úÖ LLM integration complete!\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "s9bjn9xm6df",
   "source": "### üèóÔ∏è Financial Knowledge Graph Construction\\n\\nNow we'll build a knowledge graph from our financial data, including vector embeddings for semantic search:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7z6ukmac1gl",
   "source": "# Financial Knowledge Graph Builder\\nclass FinancialGraphBuilder:\\n    \\\"\\\"\\\"Build and manage financial knowledge graph\\\"\\\"\\\"\\n    \\n    def __init__(self, neo4j_graph, llm_manager):\\n        self.kg = neo4j_graph\\n        self.llm = llm_manager\\n        self.embedding_dimension = 384  # all-MiniLM-L6-v2 dimension\\n    \\n    def clear_financial_data(self):\\n        \\\"\\\"\\\"Clear existing financial data (optional)\\\"\\\"\\\"\\n        print(\\\"üßπ Clearing existing financial data...\\\")\\n        cypher = \\\"\\\"\\\"\\n        MATCH (n) \\n        WHERE n:Company OR n:Document OR n:Section OR n:Risk OR n:Metric\\n        DETACH DELETE n\\n        \\\"\\\"\\\"\\n        self.kg.query(cypher)\\n        print(\\\"‚úÖ Financial data cleared\\\")\\n    \\n    def create_vector_index(self):\\n        \\\"\\\"\\\"Create vector index for semantic search\\\"\\\"\\\"\\n        print(\\\"üìä Creating vector index for embeddings...\\\")\\n        try:\\n            # Create vector index for document sections\\n            cypher = f\\\"\\\"\\\"\\n            CREATE VECTOR INDEX section_embeddings IF NOT EXISTS\\n            FOR (s:Section) ON (s.embedding)\\n            OPTIONS {{\\n                indexConfig: {{\\n                    `vector.dimensions`: {self.embedding_dimension},\\n                    `vector.similarity_function`: 'cosine'\\n                }}\\n            }}\\n            \\\"\\\"\\\"\\n            self.kg.query(cypher)\\n            print(\\\"‚úÖ Vector index created\\\")\\n        except Exception as e:\\n            print(f\\\"‚ö†Ô∏è Vector index creation: {e}\\\")\\n    \\n    def build_financial_graph(self, financial_documents: List[FinancialDocument]):\\n        \\\"\\\"\\\"Build the complete financial knowledge graph\\\"\\\"\\\"\\n        if not check_kg_available():\\n            print(\\\"‚ùå Neo4j connection not available\\\")\\n            return\\n        \\n        print(\\\"üèóÔ∏è Building Financial Knowledge Graph...\\\")\\n        \\n        # Create vector index first\\n        self.create_vector_index()\\n        \\n        for doc in financial_documents:\\n            print(f\\\"\\\\nüìä Processing {doc.company.name}...\\\")\\n            \\n            # 1. Create company node\\n            company_id = self._create_company_node(doc.company)\\n            \\n            # 2. Create document node\\n            doc_id = self._create_document_node(doc, company_id)\\n            \\n            # 3. Process each section\\n            for section_name, content in doc.content.items():\\n                self._create_section_node(section_name, content, doc_id, company_id)\\n            \\n            # 4. Extract and create risk nodes\\n            if 'risk_factors' in doc.content:\\n                self._extract_risks(doc.content['risk_factors'], company_id)\\n            \\n            # 5. Extract financial metrics (simplified)\\n            self._extract_metrics(doc, company_id)\\n        \\n        print(\\\"\\\\n‚úÖ Financial Knowledge Graph built successfully!\\\")\\n        self._print_graph_stats()\\n    \\n    def _create_company_node(self, company: CompanyInfo) -> str:\\n        \\\"\\\"\\\"Create company node\\\"\\\"\\\"\\n        cypher = \\\"\\\"\\\"\\n        MERGE (c:Company {ticker: $ticker})\\n        SET c.name = $name,\\n            c.cik = $cik,\\n            c.sector = $sector,\\n            c.industry = $industry,\\n            c.created_at = datetime()\\n        RETURN c.ticker AS ticker\\n        \\\"\\\"\\\"\\n        \\n        result = self.kg.query(cypher, params={\\n            'ticker': company.ticker,\\n            'name': company.name,\\n            'cik': company.cik,\\n            'sector': company.sector,\\n            'industry': company.industry\\n        })\\n        \\n        return result[0]['ticker'] if result else company.ticker\\n    \\n    def _create_document_node(self, doc: FinancialDocument, company_id: str) -> str:\\n        \\\"\\\"\\\"Create document node\\\"\\\"\\\"\\n        doc_id = f\\\"{company_id}_{doc.document_type}_{doc.filing_date}\\\"\\n        \\n        cypher = \\\"\\\"\\\"\\n        MATCH (c:Company {ticker: $company_id})\\n        MERGE (d:Document {id: $doc_id})\\n        SET d.type = $doc_type,\\n            d.filing_date = date($filing_date),\\n            d.period_end = date($period_end),\\n            d.url = $url,\\n            d.created_at = datetime()\\n        MERGE (c)-[:FILED]->(d)\\n        RETURN d.id AS doc_id\\n        \\\"\\\"\\\"\\n        \\n        self.kg.query(cypher, params={\\n            'company_id': company_id,\\n            'doc_id': doc_id,\\n            'doc_type': doc.document_type,\\n            'filing_date': doc.filing_date,\\n            'period_end': doc.period_end,\\n            'url': doc.url\\n        })\\n        \\n        return doc_id\\n    \\n    def _create_section_node(self, section_name: str, content: str, doc_id: str, company_id: str):\\n        \\\"\\\"\\\"Create section node with embeddings\\\"\\\"\\\"\\n        # Generate embedding for the content\\n        try:\\n            embedding = self.llm.get_embeddings([content])[0]\\n            embedding_list = embedding.tolist()\\n        except Exception as e:\\n            print(f\\\"‚ö†Ô∏è Could not generate embedding for {section_name}: {e}\\\")\\n            embedding_list = [0.0] * self.embedding_dimension\\n        \\n        section_id = f\\\"{doc_id}_{section_name}\\\"\\n        \\n        cypher = \\\"\\\"\\\"\\n        MATCH (d:Document {id: $doc_id})\\n        MATCH (c:Company {ticker: $company_id})\\n        MERGE (s:Section {id: $section_id})\\n        SET s.name = $section_name,\\n            s.content = $content,\\n            s.word_count = $word_count,\\n            s.embedding = $embedding,\\n            s.created_at = datetime()\\n        MERGE (d)-[:CONTAINS]->(s)\\n        MERGE (c)-[:HAS_SECTION]->(s)\\n        \\\"\\\"\\\"\\n        \\n        self.kg.query(cypher, params={\\n            'doc_id': doc_id,\\n            'company_id': company_id,\\n            'section_id': section_id,\\n            'section_name': section_name.replace('_', ' ').title(),\\n            'content': content[:5000],  # Limit content size for Neo4j\\n            'word_count': len(content.split()),\\n            'embedding': embedding_list\\n        })\\n    \\n    def _extract_risks(self, risk_content: str, company_id: str):\\n        \\\"\\\"\\\"Extract and create risk nodes from risk factors section\\\"\\\"\\\"\\n        # Simple risk extraction (in production, use NER)\\n        risk_keywords = [\\n            'competition', 'regulatory', 'cybersecurity', 'market volatility',\\n            'supply chain', 'economic conditions', 'technology changes',\\n            'legal proceedings', 'intellectual property', 'data privacy'\\n        ]\\n        \\n        content_lower = risk_content.lower()\\n        \\n        for risk_type in risk_keywords:\\n            if risk_type in content_lower:\\n                # Extract context around the risk\\n                pattern = f'.{{0,100}}{re.escape(risk_type)}.{{0,100}}'\\n                matches = re.findall(pattern, content_lower, re.IGNORECASE)\\n                \\n                if matches:\\n                    risk_context = matches[0][:300]  # First match, limited length\\n                    \\n                    cypher = \\\"\\\"\\\"\\n                    MATCH (c:Company {ticker: $company_id})\\n                    MERGE (r:Risk {type: $risk_type, company: $company_id})\\n                    SET r.description = $description,\\n                        r.severity = $severity,\\n                        r.created_at = datetime()\\n                    MERGE (c)-[:FACES_RISK]->(r)\\n                    \\\"\\\"\\\"\\n                    \\n                    # Simple severity assessment\\n                    severity = 'high' if any(word in risk_context for word in ['critical', 'significant', 'major']) else 'medium'\\n                    \\n                    self.kg.query(cypher, params={\\n                        'company_id': company_id,\\n                        'risk_type': risk_type.title(),\\n                        'description': risk_context,\\n                        'severity': severity\\n                    })\\n    \\n    def _extract_metrics(self, doc: FinancialDocument, company_id: str):\\n        \\\"\\\"\\\"Extract financial metrics (simplified)\\\"\\\"\\\"\\n        # Simple metric extraction for demo\\n        metrics = {\\n            'Revenue Growth': 'positive' if 'increased' in doc.content.get('management_discussion', '').lower() else 'unknown',\\n            'Market Position': 'strong' if 'leading' in doc.content.get('business_overview', '').lower() else 'competitive'\\n        }\\n        \\n        for metric_name, value in metrics.items():\\n            cypher = \\\"\\\"\\\"\\n            MATCH (c:Company {ticker: $company_id})\\n            MERGE (m:Metric {name: $metric_name, company: $company_id})\\n            SET m.value = $value,\\n                m.period = $period,\\n                m.created_at = datetime()\\n            MERGE (c)-[:HAS_METRIC]->(m)\\n            \\\"\\\"\\\"\\n            \\n            self.kg.query(cypher, params={\\n                'company_id': company_id,\\n                'metric_name': metric_name,\\n                'value': value,\\n                'period': doc.period_end\\n            })\\n    \\n    def _print_graph_stats(self):\\n        \\\"\\\"\\\"Print graph statistics\\\"\\\"\\\"\\n        stats_queries = {\\n            'Companies': \\\"MATCH (c:Company) RETURN count(c) AS count\\\",\\n            'Documents': \\\"MATCH (d:Document) RETURN count(d) AS count\\\",\\n            'Sections': \\\"MATCH (s:Section) RETURN count(s) AS count\\\",\\n            'Risks': \\\"MATCH (r:Risk) RETURN count(r) AS count\\\",\\n            'Metrics': \\\"MATCH (m:Metric) RETURN count(m) AS count\\\",\\n        }\\n        \\n        print(\\\"\\\\nüìà Graph Statistics:\\\")\\n        for entity, query in stats_queries.items():\\n            result = self.kg.query(query)\\n            count = result[0]['count'] if result else 0\\n            print(f\\\"   {entity}: {count}\\\")\\n\\n# Build the financial knowledge graph\\nif 'kg' in globals() and financial_documents:\\n    print(\\\"üöÄ Starting Financial Knowledge Graph construction...\\\")\\n    \\n    graph_builder = FinancialGraphBuilder(kg, llm_manager)\\n    \\n    # Optional: Clear existing financial data\\n    # Uncomment the next line to start fresh\\n    # graph_builder.clear_financial_data()\\n    \\n    # Build the graph\\n    graph_builder.build_financial_graph(financial_documents)\\nelse:\\n    print(\\\"‚ö†Ô∏è Neo4j connection or financial documents not available\\\")\\n    print(\\\"   Please ensure you've run the previous cells successfully\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ja0wv9r66d",
   "source": "### üí¨ GraphRAG Investment Advisor\\n\\nNow we'll create an intelligent investment advisor that uses our knowledge graph and LLM to provide financial insights:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0y7a0zu0z55l",
   "source": "# GraphRAG Investment Advisor\\nclass InvestmentAdvisor:\\n    \\\"\\\"\\\"AI-powered investment advisor using GraphRAG\\\"\\\"\\\"\\n    \\n    def __init__(self, neo4j_graph, llm_manager):\\n        self.kg = neo4j_graph\\n        self.llm = llm_manager\\n        self.embedding_dimension = 384\\n    \\n    def semantic_search(self, query: str, limit: int = 5) -> List[Dict]:\\n        \\\"\\\"\\\"Semantic search using vector embeddings\\\"\\\"\\\"\\n        try:\\n            # Get query embedding\\n            query_embedding = self.llm.get_embeddings([query])[0].tolist()\\n            \\n            # Search for similar sections\\n            cypher = \\\"\\\"\\\"\\n            CALL db.index.vector.queryNodes('section_embeddings', $limit, $query_embedding) \\n            YIELD node AS section, score\\n            MATCH (section)<-[:HAS_SECTION]-(company:Company)\\n            RETURN \\n                company.name AS company,\\n                company.ticker AS ticker,\\n                section.name AS section_type,\\n                section.content AS content,\\n                score,\\n                section.word_count AS word_count\\n            ORDER BY score DESC\\n            \\\"\\\"\\\"\\n            \\n            result = self.kg.query(cypher, params={\\n                'query_embedding': query_embedding,\\n                'limit': limit\\n            })\\n            \\n            return result\\n            \\n        except Exception as e:\\n            print(f\\\"‚ö†Ô∏è Semantic search error: {e}\\\")\\n            return self._fallback_search(query, limit)\\n    \\n    def _fallback_search(self, query: str, limit: int = 5) -> List[Dict]:\\n        \\\"\\\"\\\"Fallback text search when vector search isn't available\\\"\\\"\\\"\\n        query_lower = query.lower()\\n        \\n        cypher = \\\"\\\"\\\"\\n        MATCH (c:Company)-[:HAS_SECTION]->(s:Section)\\n        WHERE toLower(s.content) CONTAINS $query\\n           OR toLower(s.name) CONTAINS $query\\n           OR toLower(c.name) CONTAINS $query\\n        RETURN \\n            c.name AS company,\\n            c.ticker AS ticker,\\n            s.name AS section_type,\\n            s.content AS content,\\n            0.8 AS score,  // Dummy score for fallback\\n            s.word_count AS word_count\\n        LIMIT $limit\\n        \\\"\\\"\\\"\\n        \\n        return self.kg.query(cypher, params={'query': query_lower, 'limit': limit})\\n    \\n    def get_company_overview(self, ticker: str) -> Dict:\\n        \\\"\\\"\\\"Get comprehensive company overview\\\"\\\"\\\"\\n        cypher = \\\"\\\"\\\"\\n        MATCH (c:Company {ticker: $ticker})\\n        OPTIONAL MATCH (c)-[:FACES_RISK]->(r:Risk)\\n        OPTIONAL MATCH (c)-[:HAS_METRIC]->(m:Metric)\\n        OPTIONAL MATCH (c)-[:HAS_SECTION]->(s:Section)\\n        RETURN \\n            c.name AS company_name,\\n            c.sector AS sector,\\n            c.industry AS industry,\\n            COLLECT(DISTINCT r.type) AS risks,\\n            COLLECT(DISTINCT {name: m.name, value: m.value}) AS metrics,\\n            COLLECT(DISTINCT s.name) AS available_sections\\n        \\\"\\\"\\\"\\n        \\n        result = self.kg.query(cypher, params={'ticker': ticker.upper()})\\n        return result[0] if result else {}\\n    \\n    def analyze_risks(self, ticker: str = None) -> List[Dict]:\\n        \\\"\\\"\\\"Analyze risks for a company or across all companies\\\"\\\"\\\"\\n        if ticker:\\n            cypher = \\\"\\\"\\\"\\n            MATCH (c:Company {ticker: $ticker})-[:FACES_RISK]->(r:Risk)\\n            RETURN \\n                c.name AS company,\\n                r.type AS risk_type,\\n                r.severity AS severity,\\n                r.description AS description\\n            ORDER BY \\n                CASE r.severity WHEN 'high' THEN 1 WHEN 'medium' THEN 2 ELSE 3 END,\\n                r.type\\n            \\\"\\\"\\\"\\n            params = {'ticker': ticker.upper()}\\n        else:\\n            cypher = \\\"\\\"\\\"\\n            MATCH (c:Company)-[:FACES_RISK]->(r:Risk)\\n            RETURN \\n                c.name AS company,\\n                r.type AS risk_type,\\n                r.severity AS severity,\\n                r.description AS description\\n            ORDER BY \\n                CASE r.severity WHEN 'high' THEN 1 WHEN 'medium' THEN 2 ELSE 3 END,\\n                c.name\\n            \\\"\\\"\\\"\\n            params = {}\\n        \\n        return self.kg.query(cypher, params=params)\\n    \\n    def compare_companies(self, tickers: List[str]) -> Dict:\\n        \\\"\\\"\\\"Compare multiple companies across key metrics\\\"\\\"\\\"\\n        ticker_list = [t.upper() for t in tickers]\\n        \\n        cypher = \\\"\\\"\\\"\\n        MATCH (c:Company)\\n        WHERE c.ticker IN $tickers\\n        OPTIONAL MATCH (c)-[:FACES_RISK]->(r:Risk)\\n        OPTIONAL MATCH (c)-[:HAS_METRIC]->(m:Metric)\\n        RETURN \\n            c.ticker AS ticker,\\n            c.name AS company_name,\\n            c.sector AS sector,\\n            COUNT(DISTINCT r) AS total_risks,\\n            COUNT(DISTINCT CASE WHEN r.severity = 'high' THEN r END) AS high_risks,\\n            COLLECT(DISTINCT r.type) AS risk_types,\\n            COLLECT(DISTINCT {name: m.name, value: m.value}) AS metrics\\n        ORDER BY c.ticker\\n        \\\"\\\"\\\"\\n        \\n        results = self.kg.query(cypher, params={'tickers': ticker_list})\\n        \\n        # Structure the comparison\\n        comparison = {\\n            'companies': results,\\n            'summary': self._generate_comparison_summary(results)\\n        }\\n        \\n        return comparison\\n    \\n    def _generate_comparison_summary(self, company_data: List[Dict]) -> str:\\n        \\\"\\\"\\\"Generate a summary of company comparison\\\"\\\"\\\"\\n        if not company_data:\\n            return \\\"No companies found for comparison.\\\"\\n        \\n        # Simple rule-based summary\\n        lowest_risk = min(company_data, key=lambda x: x['total_risks'])\\n        highest_risk = max(company_data, key=lambda x: x['total_risks'])\\n        \\n        summary = f\\\"\\\"\\\"\\n        Company Comparison Summary:\\n        ‚Ä¢ Lowest Risk: {lowest_risk['company_name']} ({lowest_risk['total_risks']} risks)\\n        ‚Ä¢ Highest Risk: {highest_risk['company_name']} ({highest_risk['total_risks']} risks)\\n        ‚Ä¢ All companies operate in similar sectors: {', '.join(set(c['sector'] for c in company_data))}\\n        \\\"\\\"\\\"\\n        \\n        return summary.strip()\\n    \\n    def ask_advisor(self, question: str) -> str:\\n        \\\"\\\"\\\"Main advisor interface - answer investment questions using GraphRAG\\\"\\\"\\\"\\n        print(f\\\"ü§î Question: {question}\\\")\\n        print(\\\"üîç Searching knowledge graph...\\\")\\n        \\n        # Perform semantic search\\n        relevant_sections = self.semantic_search(question, limit=3)\\n        \\n        if not relevant_sections:\\n            return \\\"I couldn't find relevant information in the financial database. Please try a more specific question about the companies in our database.\\\"\\n        \\n        # Build context from search results\\n        context = \\\"\\\\n\\\\n\\\".join([\\n            f\\\"**{section['company']} ({section['ticker']}) - {section['section_type']}:**\\\\n{section['content'][:500]}...\\\"\\n            for section in relevant_sections\\n        ])\\n        \\n        # Generate response using LLM with context\\n        prompt = f\\\"\\\"\\\"\\n        You are a financial investment advisor. Based on the following financial information from SEC filings, \\n        please provide a helpful and informative response to the investor's question.\\n        \\n        Question: {question}\\n        \\n        Relevant Financial Information:\\n        {context}\\n        \\n        Please provide:\\n        1. A direct answer to the question\\n        2. Key insights from the financial data\\n        3. Important considerations for investors\\n        4. Any relevant risks or opportunities\\n        \\n        Response:\\n        \\\"\\\"\\\"\\n        \\n        response = self.llm.generate_text(prompt, max_length=300)\\n        \\n        return response\\n    \\n    def investment_screening(self, criteria: Dict) -> List[Dict]:\\n        \\\"\\\"\\\"Screen companies based on investment criteria\\\"\\\"\\\"\\n        # Build dynamic query based on criteria\\n        where_conditions = []\\n        params = {}\\n        \\n        if 'sector' in criteria:\\n            where_conditions.append(\\\"c.sector = $sector\\\")\\n            params['sector'] = criteria['sector']\\n        \\n        if 'max_risks' in criteria:\\n            where_conditions.append(\\\"riskCount <= $max_risks\\\")\\n            params['max_risks'] = criteria['max_risks']\\n        \\n        where_clause = \\\" AND \\\".join(where_conditions) if where_conditions else \\\"true\\\"\\n        \\n        cypher = f\\\"\\\"\\\"\\n        MATCH (c:Company)\\n        OPTIONAL MATCH (c)-[:FACES_RISK]->(r:Risk)\\n        WITH c, COUNT(r) AS riskCount\\n        WHERE {where_clause}\\n        RETURN \\n            c.ticker AS ticker,\\n            c.name AS company_name,\\n            c.sector AS sector,\\n            c.industry AS industry,\\n            riskCount AS total_risks\\n        ORDER BY riskCount ASC, c.ticker\\n        \\\"\\\"\\\"\\n        \\n        return self.kg.query(cypher, params=params)\\n\\n# Initialize the Investment Advisor\\nif 'kg' in globals() and 'llm_manager' in globals():\\n    advisor = InvestmentAdvisor(kg, llm_manager)\\n    print(\\\"üéØ Investment Advisor initialized successfully!\\\")\\n    \\n    # Show available companies\\n    print(\\\"\\\\nüìã Available companies for analysis:\\\")\\n    companies = advisor.kg.query(\\\"MATCH (c:Company) RETURN c.ticker AS ticker, c.name AS name ORDER BY c.ticker\\\")\\n    for company in companies:\\n        print(f\\\"   {company['ticker']}: {company['name']}\\\")\\nelse:\\n    print(\\\"‚ö†Ô∏è Cannot initialize advisor - missing dependencies\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ui39ccugkng",
   "source": "### üé™ Demo: Financial Investment Advisor in Action\\n\\nLet's test our GraphRAG investment advisor with real financial questions:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "180l32ndf5n",
   "source": "# Demo: Investment Advisor in Action\\nif 'advisor' in globals():\\n    print(\\\"üé™ GraphRAG Financial Investment Advisor Demo\\\")\\n    print(\\\"=\\\" * 60)\\n    \\n    # Demo 1: Company Overview\\n    print(\\\"\\\\n1Ô∏è‚É£ Company Overview Analysis\\\")\\n    print(\\\"-\\\" * 35)\\n    overview = advisor.get_company_overview('AAPL')\\n    if overview:\\n        print(f\\\"Company: {overview.get('company_name', 'N/A')}\\\")\\n        print(f\\\"Sector: {overview.get('sector', 'N/A')}\\\")\\n        print(f\\\"Industry: {overview.get('industry', 'N/A')}\\\")\\n        print(f\\\"Identified Risks: {', '.join(overview.get('risks', []))}\\\")\\n        print(f\\\"Available Sections: {', '.join(overview.get('available_sections', []))}\\\")\\n    else:\\n        print(\\\"No company data found\\\")\\n    \\n    # Demo 2: Risk Analysis\\n    print(\\\"\\\\n2Ô∏è‚É£ Risk Analysis\\\")\\n    print(\\\"-\\\" * 20)\\n    risks = advisor.analyze_risks('AAPL')\\n    if risks:\\n        for risk in risks[:3]:  # Show first 3 risks\\n            print(f\\\"üö® {risk['risk_type']} ({risk['severity']} severity)\\\")\\n            print(f\\\"   {risk['description'][:100]}...\\\")\\n    else:\\n        print(\\\"No risks found\\\")\\n    \\n    # Demo 3: Company Comparison\\n    print(\\\"\\\\n3Ô∏è‚É£ Company Comparison\\\")\\n    print(\\\"-\\\" * 25)\\n    comparison = advisor.compare_companies(['AAPL', 'MSFT'])\\n    if comparison['companies']:\\n        print(\\\"üìä Comparison Results:\\\")\\n        for company in comparison['companies']:\\n            print(f\\\"   {company['ticker']}: {company['total_risks']} risks, {company['high_risks']} high-severity\\\")\\n        print(f\\\"\\\\nüí° {comparison['summary']}\\\")\\n    \\n    # Demo 4: Investment Screening\\n    print(\\\"\\\\n4Ô∏è‚É£ Investment Screening\\\")\\n    print(\\\"-\\\" * 25)\\n    criteria = {'sector': 'Technology', 'max_risks': 5}\\n    screened = advisor.investment_screening(criteria)\\n    if screened:\\n        print(f\\\"Companies matching criteria (Technology sector, ‚â§5 risks):\\\")\\n        for company in screened:\\n            print(f\\\"   ‚úÖ {company['ticker']}: {company['company_name']} ({company['total_risks']} risks)\\\")\\n    \\n    # Demo 5: Ask the Advisor (Main GraphRAG feature)\\n    print(\\\"\\\\n5Ô∏è‚É£ AI Investment Advisor (GraphRAG)\\\")\\n    print(\\\"-\\\" * 40)\\n    \\n    # Example questions\\n    questions = [\\n        \\\"What are the main risks facing Apple?\\\",\\n        \\\"Should I invest in technology companies?\\\",\\n        \\\"How is Microsoft's business performing?\\\"\\n    ]\\n    \\n    for question in questions[:2]:  # Demo first 2 questions\\n        print(f\\\"\\\\n‚ùì Question: {question}\\\")\\n        print(\\\"ü§ñ AI Response:\\\")\\n        response = advisor.ask_advisor(question)\\n        print(f\\\"   {response[:300]}...\\\")  # Show first 300 characters\\n        print()\\n    \\n    print(\\\"\\\\n‚ú® Try asking your own questions about the companies in our database!\\\")\\n    print(\\\"   Available companies: AAPL, MSFT (expand with more SEC data)\\\")\\n    \\nelse:\\n    print(\\\"‚ùå Investment Advisor not available\\\")\\n    print(\\\"Please run the previous cells to initialize the advisor\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f15fwcxvj0s",
   "source": "### üéØ Section Summary and Next Steps\\n\\n**üéâ Congratulations!** You've built a complete GraphRAG Financial Investment Advisor using open-source technologies!\\n\\n#### ‚úÖ **What You've Accomplished:**\\n- **üìä SEC Data Integration**: Real financial data extraction and processing\\n- **ü§ñ Open Source AI**: LLM integration with Ollama/HuggingFace fallbacks\\n- **üèóÔ∏è Knowledge Graph**: Financial entities, relationships, and vector embeddings\\n- **üí¨ GraphRAG System**: Semantic search + AI-generated insights\\n- **üéØ Business Application**: Practical investment advisor with real value\\n\\n#### üöÄ **Production Enhancements:**\\n1. **Real SEC API Integration**: Replace demo data with live SEC EDGAR feeds\\n2. **Advanced NER**: Use spaCy or custom models for better entity extraction\\n3. **Financial Metrics**: Integrate quantitative financial ratios and calculations\\n4. **News Integration**: Add real-time financial news and sentiment analysis\\n5. **User Interface**: Build a web app with Streamlit or FastAPI\\n6. **Alerts System**: Set up notifications for risk changes or opportunities\\n\\n#### üîß **Technical Improvements:**\\n1. **Scaling**: Implement data pipelines for larger datasets\\n2. **Performance**: Optimize vector searches and caching\\n3. **Security**: Add user authentication and data privacy controls\\n4. **Monitoring**: Add logging, metrics, and error tracking\\n\\n#### üí° **Business Extensions:**\\n1. **Portfolio Management**: Track and analyze investment portfolios\\n2. **Risk Modeling**: Quantitative risk assessment and VaR calculations\\n3. **ESG Analysis**: Environmental, social, and governance scoring\\n4. **Regulatory Compliance**: Automated compliance checking and reporting\\n\\n---\\n\\n**üéä You now have a working GraphRAG system that demonstrates the power of combining Knowledge Graphs with AI for real-world business applications!**\"",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "1853mhdt02p",
   "source": "## 12. Index Management and Query Profiling\\n\\n### üìá Understanding Neo4j Indexes\\n\\nIndexes in Neo4j dramatically improve query performance by creating fast lookup structures for node properties and relationships.\\n\\n#### üîç Types of Indexes:\\n- **Range Index**: For exact matches and range queries (numbers, dates, strings)\\n- **Text Index**: For full-text search capabilities\\n- **Point Index**: For spatial/geographic data\\n- **Composite Index**: Combines multiple properties\\n\\n#### üìä When to Create Indexes:\\n- Properties used frequently in `MATCH` clauses\\n- Properties in `WHERE` conditions\\n- Properties used for `ORDER BY`\\n- Unique constraints (automatically create indexes)\\n\\nLet's demonstrate index creation and performance analysis:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w0ekaizrsre",
   "source": "# Index Management Demonstration\\nif not check_kg_available():\\n    print(\\\"‚è∏Ô∏è Skipping index demo - connection not available\\\")\\nelse:\\n    print(\\\"üìá Index Management and Performance Analysis\\\")\\n    print(\\\"=\\\" * 50)\\n    \\n    # Step 1: Check existing indexes\\n    print(\\\"\\\\n1Ô∏è‚É£ Current Database Indexes:\\\")\\n    try:\\n        # Try modern SHOW INDEXES command\\n        result = kg.query(\\\"SHOW INDEXES\\\")\\n        if result:\\n            print(f\\\"   Found {len(result)} existing indexes:\\\")\\n            for idx in result[:5]:  # Show first 5\\n                name = idx.get('name', 'unnamed')\\n                labels = idx.get('labelsOrTypes', [])\\n                properties = idx.get('properties', [])\\n                state = idx.get('state', 'unknown')\\n                print(f\\\"   üìã {name}: {labels} - {properties} ({state})\\\")\\n        else:\\n            print(\\\"   ‚ÑπÔ∏è No indexes found\\\")\\n    except Exception as e:\\n        # Fallback for older Neo4j versions\\n        print(f\\\"   ‚ö†Ô∏è Could not retrieve indexes: {str(e)[:50]}...\\\")\\n        print(\\\"   (This may be due to Neo4j version or permissions)\\\")\\n    \\n    # Step 2: Create useful indexes for our movie data\\n    print(\\\"\\\\n2Ô∏è‚É£ Creating Performance Indexes:\\\")\\n    \\n    indexes_to_create = [\\n        {\\n            \\\"name\\\": \\\"person_name_index\\\",\\n            \\\"command\\\": \\\"CREATE INDEX person_name_index IF NOT EXISTS FOR (p:Person) ON (p.name)\\\",\\n            \\\"description\\\": \\\"Index on Person.name for faster actor/director lookups\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"movie_title_index\\\", \\n            \\\"command\\\": \\\"CREATE INDEX movie_title_index IF NOT EXISTS FOR (m:Movie) ON (m.title)\\\",\\n            \\\"description\\\": \\\"Index on Movie.title for faster movie searches\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"movie_year_index\\\",\\n            \\\"command\\\": \\\"CREATE INDEX movie_year_index IF NOT EXISTS FOR (m:Movie) ON (m.released)\\\",\\n            \\\"description\\\": \\\"Index on Movie.released for year-based queries\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"genre_name_index\\\",\\n            \\\"command\\\": \\\"CREATE INDEX genre_name_index IF NOT EXISTS FOR (g:Genre) ON (g.name)\\\", \\n            \\\"description\\\": \\\"Index on Genre.name for genre-based filtering\\\"\\n        }\\n    ]\\n    \\n    for idx in indexes_to_create:\\n        try:\\n            kg.query(idx[\\\"command\\\"])\\n            print(f\\\"   ‚úÖ {idx['name']}: {idx['description']}\\\")\\n        except Exception as e:\\n            print(f\\\"   ‚ö†Ô∏è {idx['name']}: {str(e)[:60]}...\\\")\\n    \\n    print(\\\"\\\\n   üí° Indexes may take a moment to build in the background\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "13gdjcpxw63c",
   "source": "### üìä Query Profiling with EXPLAIN and PROFILE\\n\\n**EXPLAIN** shows the query execution plan without running the query, while **PROFILE** executes the query and shows actual performance metrics.\\n\\n#### Key Metrics to Watch:\\n- **db hits**: Number of operations against the graph store\\n- **rows**: Number of rows processed at each step\\n- **time**: Actual execution time (PROFILE only)\\n- **memory**: Memory usage (PROFILE only)\\n- **Index usage**: Whether indexes are being utilized\\n\\nLet's compare query performance with and without indexes:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "f28hcft0k55",
   "source": "# Query Profiling and Performance Analysis\\nif not check_kg_available():\\n    print(\\\"‚è∏Ô∏è Skipping profiling demo - connection not available\\\")\\nelse:\\n    print(\\\"üìä Query Profiling and Performance Analysis\\\")\\n    print(\\\"=\\\" * 50)\\n    \\n    # Helper function to format profiling results\\n    def format_profile_info(result):\\n        if not result or len(result) == 0:\\n            return \\\"No profile data available\\\"\\n        \\n        # Extract key metrics from profile\\n        info = []\\n        for row in result:\\n            for key, value in row.items():\\n                if isinstance(value, str) and ('db hits' in value.lower() or 'rows' in value.lower()):\\n                    info.append(value[:100])  # Truncate long strings\\n        return \\\"\\\\n   \\\".join(info[:3]) if info else \\\"Profile data format not recognized\\\"\\n    \\n    # Demo 1: EXPLAIN - Show execution plan without running\\n    print(\\\"\\\\n1Ô∏è‚É£ EXPLAIN - Query Execution Plan (without execution):\\\")\\n    print(\\\"   Query: Find Tom Hanks movies\\\")\\n    \\n    try:\\n        explain_result = kg.query(\\\"\\\"\\\"\\n        EXPLAIN \\n        MATCH (p:Person {name: \\\"Tom Hanks\\\"})-[:ACTED_IN]->(m:Movie)\\n        RETURN p.name, m.title, m.released\\n        \\\"\\\"\\\")\\n        \\n        print(\\\"   üìã Execution Plan:\\\")\\n        if explain_result:\\n            for i, step in enumerate(explain_result[:3]):  # Show first 3 steps\\n                print(f\\\"   Step {i+1}: {list(step.keys())[0] if step else 'Unknown'}\\\")\\n        else:\\n            print(\\\"   ‚ö†Ô∏è No execution plan data returned\\\")\\n            \\n    except Exception as e:\\n        print(f\\\"   ‚ö†Ô∏è EXPLAIN failed: {str(e)[:60]}...\\\")\\n    \\n    # Demo 2: PROFILE - Show actual performance metrics\\n    print(\\\"\\\\n2Ô∏è‚É£ PROFILE - Actual Performance Metrics:\\\")\\n    print(\\\"   Query: Find all Crime movies with their actors\\\")\\n    \\n    import time\\n    start_time = time.time()\\n    \\n    try:\\n        profile_result = kg.query(\\\"\\\"\\\"\\n        PROFILE\\n        MATCH (m:Movie)-[:BELONGS_TO]->(g:Genre {name: \\\"Crime\\\"})\\n        MATCH (m)<-[:ACTED_IN]-(a:Person)\\n        RETURN m.title AS movie, m.released AS year, collect(a.name)[0..3] AS actors\\n        ORDER BY m.released DESC\\n        \\\"\\\"\\\")\\n        \\n        execution_time = time.time() - start_time\\n        \\n        print(f\\\"   ‚è±Ô∏è Query executed in {execution_time:.4f} seconds\\\")\\n        print(f\\\"   üìä Returned {len(profile_result)} rows\\\")\\n        \\n        # Show sample results\\n        print(\\\"\\\\n   üìã Sample Results:\\\")\\n        for row in profile_result[:2]:  # Show first 2 movies\\n            actors_str = \\\", \\\".join(row['actors'][:3])\\n            print(f\\\"   üé¨ {row['movie']} ({row['year']}) - {actors_str}...\\\")\\n            \\n        print(\\\"\\\\n   üìà Performance Profile Available (detailed metrics in Neo4j Browser)\\\")\\n        \\n    except Exception as e:\\n        print(f\\\"   ‚ö†Ô∏è PROFILE failed: {str(e)[:60]}...\\\")\\n    \\n    # Demo 3: Compare indexed vs non-indexed performance\\n    print(\\\"\\\\n3Ô∏è‚É£ Performance Comparison - Index Usage:\\\")\\n    \\n    # Query that should use Person.name index\\n    queries_to_compare = [\\n        {\\n            \\\"name\\\": \\\"Indexed Property Search\\\",\\n            \\\"query\\\": \\\"MATCH (p:Person {name: 'Leonardo DiCaprio'}) RETURN p.name, p.born\\\",\\n            \\\"explanation\\\": \\\"Uses index on Person.name (should be fast)\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"Range Query on Indexed Property\\\", \\n            \\\"query\\\": \\\"MATCH (m:Movie) WHERE m.released >= 2000 AND m.released <= 2010 RETURN m.title, m.released ORDER BY m.released\\\",\\n            \\\"explanation\\\": \\\"Uses index on Movie.released for range query\\\"\\n        },\\n        {\\n            \\\"name\\\": \\\"Non-indexed Property Filter\\\",\\n            \\\"query\\\": \\\"MATCH (m:Movie) WHERE m.tagline CONTAINS 'you' RETURN m.title, m.tagline LIMIT 3\\\",\\n            \\\"explanation\\\": \\\"No index on tagline - requires full scan\\\"\\n        }\\n    ]\\n    \\n    for i, query_info in enumerate(queries_to_compare):\\n        print(f\\\"\\\\n   Query {i+1}: {query_info['name']}\\\")\\n        print(f\\\"   üí° {query_info['explanation']}\\\")\\n        \\n        start_time = time.time()\\n        try:\\n            result = kg.query(query_info['query'])\\n            execution_time = time.time() - start_time\\n            \\n            print(f\\\"   ‚è±Ô∏è Executed in {execution_time:.4f} seconds ({len(result)} rows)\\\")\\n            \\n            # Show first result if available\\n            if result:\\n                first_result = result[0]\\n                result_str = \\\", \\\".join([f\\\"{k}: {v}\\\" for k, v in first_result.items()])\\n                print(f\\\"   üìã Sample: {result_str[:80]}...\\\")\\n                \\n        except Exception as e:\\n            print(f\\\"   ‚ùå Query failed: {str(e)[:50]}...\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "eaj6n9rux8u",
   "source": "### üî¨ Advanced Profiling Techniques\\n\\nFor deeper performance analysis, Neo4j provides additional profiling capabilities:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "pm78mc39ije",
   "source": "# Advanced Profiling Techniques\\nif not check_kg_available():\\n    print(\\\"‚è∏Ô∏è Skipping advanced profiling - connection not available\\\")\\nelse:\\n    print(\\\"üî¨ Advanced Profiling Techniques\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    # Demo 1: Memory profiling for large result sets\\n    print(\\\"\\\\n1Ô∏è‚É£ Memory Usage Analysis:\\\")\\n    print(\\\"   Comparing memory-efficient vs memory-intensive queries\\\")\\n    \\n    # Memory-efficient query (using aggregation)\\n    print(\\\"\\\\n   Memory-Efficient Query (Aggregation):\\\")\\n    start_time = time.time()\\n    try:\\n        result1 = kg.query(\\\"\\\"\\\"\\n        MATCH (g:Genre)<-[:BELONGS_TO]-(m:Movie)\\n        RETURN g.name AS genre, count(m) AS movieCount\\n        ORDER BY movieCount DESC\\n        \\\"\\\"\\\")\\n        time1 = time.time() - start_time\\n        print(f\\\"   ‚è±Ô∏è Aggregation query: {time1:.4f} seconds, {len(result1)} rows\\\")\\n        print(f\\\"   üíæ Low memory usage (only final aggregated results)\\\")\\n        \\n    except Exception as e:\\n        print(f\\\"   ‚ùå Query failed: {str(e)[:50]}...\\\")\\n    \\n    # Memory-intensive query (collecting all items)\\n    print(\\\"\\\\n   Memory-Intensive Query (Collection):\\\")\\n    start_time = time.time()\\n    try:\\n        result2 = kg.query(\\\"\\\"\\\"\\n        MATCH (g:Genre)<-[:BELONGS_TO]-(m:Movie)\\n        RETURN g.name AS genre, collect(m.title) AS allMovies\\n        ORDER BY g.name\\n        \\\"\\\"\\\")\\n        time2 = time.time() - start_time\\n        print(f\\\"   ‚è±Ô∏è Collection query: {time2:.4f} seconds, {len(result2)} rows\\\")\\n        print(f\\\"   üíæ Higher memory usage (stores all movie titles in memory)\\\")\\n        \\n        # Show memory impact\\n        if result2:\\n            total_movies = sum(len(row['allMovies']) for row in result2)\\n            print(f\\\"   üìä Total items in memory: {total_movies} movie titles\\\")\\n            \\n    except Exception as e:\\n        print(f\\\"   ‚ùå Query failed: {str(e)[:50]}...\\\")\\n    \\n    # Demo 2: Index hit analysis\\n    print(\\\"\\\\n2Ô∏è‚É£ Index Effectiveness Analysis:\\\")\\n    \\n    # Function to simulate index usage analysis\\n    def analyze_query_performance(query_name, query, expected_index_usage=True):\\n        print(f\\\"\\\\n   üîç {query_name}:\\\")\\n        start_time = time.time()\\n        try:\\n            result = kg.query(query)\\n            execution_time = time.time() - start_time\\n            \\n            rows_returned = len(result) if result else 0\\n            print(f\\\"   ‚è±Ô∏è Time: {execution_time:.4f}s, Rows: {rows_returned}\\\")\\n            \\n            if expected_index_usage:\\n                if execution_time < 0.01:  # Very fast suggests index usage\\n                    print(f\\\"   ‚úÖ Likely using index (very fast execution)\\\")\\n                elif execution_time < 0.05:\\n                    print(f\\\"   ‚ö° Possibly using index (fast execution)\\\")\\n                else:\\n                    print(f\\\"   ‚ö†Ô∏è May not be using index (slower execution)\\\")\\n            else:\\n                print(f\\\"   üìä Full scan expected (no suitable index)\\\")\\n                \\n        except Exception as e:\\n            print(f\\\"   ‚ùå Query failed: {str(e)[:50]}...\\\")\\n    \\n    # Test different query patterns\\n    analyze_query_performance(\\n        \\\"Exact Match (Should Use Index)\\\",\\n        \\\"MATCH (p:Person {name: 'Tom Hanks'}) RETURN p\\\",\\n        expected_index_usage=True\\n    )\\n    \\n    analyze_query_performance(\\n        \\\"Range Query (Should Use Index)\\\", \\n        \\\"MATCH (m:Movie) WHERE m.released >= 2000 RETURN m.title, m.released LIMIT 5\\\",\\n        expected_index_usage=True\\n    )\\n    \\n    analyze_query_performance(\\n        \\\"Pattern Match (May Not Use Index)\\\",\\n        \\\"MATCH (p:Person) WHERE p.name STARTS WITH 'Tom' RETURN p.name LIMIT 5\\\",\\n        expected_index_usage=False\\n    )\\n    \\n    analyze_query_performance(\\n        \\\"Full Property Scan (No Index)\\\",\\n        \\\"MATCH (m:Movie) WHERE m.tagline CONTAINS 'the' RETURN m.title, m.tagline LIMIT 3\\\",\\n        expected_index_usage=False\\n    )\\n    \\n    # Demo 3: Query optimization suggestions\\n    print(\\\"\\\\n3Ô∏è‚É£ Query Optimization Tips:\\\")\\n    optimization_tips = [\\n        \\\"‚úÖ Use specific labels in MATCH clauses: MATCH (p:Person) not MATCH (p)\\\",\\n        \\\"‚úÖ Start with most selective patterns: specific nodes before broad patterns\\\",\\n        \\\"‚úÖ Use indexed properties for filtering: WHERE p.name = 'value'\\\",\\n        \\\"‚úÖ Apply LIMIT early: add LIMIT before expensive operations\\\",\\n        \\\"‚úÖ Use DISTINCT sparingly: only when absolutely necessary\\\",\\n        \\\"‚úÖ Prefer aggregation over collection: count() instead of collect()\\\",\\n        \\\"‚úÖ Use OPTIONAL MATCH for optional relationships\\\",\\n        \\\"‚úÖ Create composite indexes for multi-property queries\\\"\\n    ]\\n    \\n    for tip in optimization_tips:\\n        print(f\\\"   {tip}\\\")\\n    \\n    print(\\\"\\\\nüí° Pro Tip: Use Neo4j Browser for visual query profiling!\\\")\\n    print(\\\"   ‚Ä¢ Open Neo4j Browser at your database URL\\\")\\n    print(\\\"   ‚Ä¢ Prefix queries with PROFILE or EXPLAIN\\\")\\n    print(\\\"   ‚Ä¢ View execution plans and performance metrics graphically\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "kiiixtxydg",
   "source": "### üßπ Index Maintenance and Cleanup\\n\\nProper index management includes monitoring and cleanup:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "w8jlrlmpdjg",
   "source": "# Index Maintenance and Management\\nif not check_kg_available():\\n    print(\\\"‚è∏Ô∏è Skipping index maintenance demo - connection not available\\\")\\nelse:\\n    print(\\\"üßπ Index Maintenance and Management\\\")\\n    print(\\\"=\\\" * 40)\\n    \\n    # Demo 1: Monitor index usage and health\\n    print(\\\"\\\\n1Ô∏è‚É£ Index Health Check:\\\")\\n    \\n    try:\\n        # Check index status\\n        result = kg.query(\\\"SHOW INDEXES\\\")\\n        if result:\\n            print(f\\\"   üìä Found {len(result)} indexes:\\\")\\n            \\n            # Categorize indexes by state\\n            index_states = {}\\n            for idx in result:\\n                state = idx.get('state', 'unknown')\\n                index_states[state] = index_states.get(state, 0) + 1\\n            \\n            for state, count in index_states.items():\\n                status_emoji = \\\"‚úÖ\\\" if state.lower() == \\\"online\\\" else \\\"‚ö†Ô∏è\\\"\\n                print(f\\\"   {status_emoji} {state}: {count} indexes\\\")\\n                \\n            # Show any failed indexes\\n            failed_indexes = [idx for idx in result if idx.get('state', '').lower() == 'failed']\\n            if failed_indexes:\\n                print(\\\"\\\\n   ‚ùå Failed indexes found:\\\")\\n                for idx in failed_indexes:\\n                    print(f\\\"     - {idx.get('name', 'unnamed')}: {idx.get('failureMessage', 'Unknown error')}\\\")\\n            \\n        else:\\n            print(\\\"   ‚ÑπÔ∏è No indexes found or unable to retrieve index information\\\")\\n            \\n    except Exception as e:\\n        print(f\\\"   ‚ö†Ô∏è Could not check index health: {str(e)[:60]}...\\\")\\n    \\n    # Demo 2: Example of dropping unused indexes (commented for safety)\\n    print(\\\"\\\\n2Ô∏è‚É£ Index Cleanup (Educational Example):\\\")\\n    print(\\\"   üí° In production, regularly review and remove unused indexes\\\")\\n    \\n    cleanup_examples = [\\n        {\\n            \\\"scenario\\\": \\\"Remove index on rarely queried property\\\",\\n            \\\"command\\\": \\\"DROP INDEX movie_tagline_index IF EXISTS\\\",\\n            \\\"reason\\\": \\\"Tagline is rarely used in WHERE clauses\\\"\\n        },\\n        {\\n            \\\"scenario\\\": \\\"Drop duplicate indexes\\\", \\n            \\\"command\\\": \\\"DROP INDEX old_person_name_index IF EXISTS\\\",\\n            \\\"reason\\\": \\\"Replaced by composite index on (name, born)\\\"\\n        },\\n        {\\n            \\\"scenario\\\": \\\"Remove index on changed schema\\\",\\n            \\\"command\\\": \\\"DROP INDEX deprecated_property_index IF EXISTS\\\",\\n            \\\"reason\\\": \\\"Property no longer exists in data model\\\"\\n        }\\n    ]\\n    \\n    for example in cleanup_examples:\\n        print(f\\\"\\\\n   üóëÔ∏è {example['scenario']}:\\\")\\n        print(f\\\"      Command: {example['command']}\\\")\\n        print(f\\\"      Reason: {example['reason']}\\\")\\n    \\n    print(\\\"\\\\n   ‚ö†Ô∏è Note: Above commands are examples only - not executed for safety\\\")\\n    \\n    # Demo 3: Index performance recommendations\\n    print(\\\"\\\\n3Ô∏è‚É£ Index Performance Recommendations:\\\")\\n    \\n    recommendations = [\\n        {\\n            \\\"category\\\": \\\"üéØ Essential Indexes\\\",\\n            \\\"items\\\": [\\n                \\\"Create indexes on frequently filtered properties (name, id, status)\\\",\\n                \\\"Index properties used in JOIN-like operations (relationship endpoints)\\\",\\n                \\\"Index properties used in ORDER BY clauses\\\"\\n            ]\\n        },\\n        {\\n            \\\"category\\\": \\\"‚ö° Performance Indexes\\\", \\n            \\\"items\\\": [\\n                \\\"Composite indexes for multi-property queries\\\",\\n                \\\"Range indexes for date/numeric range queries\\\",\\n                \\\"Text indexes for full-text search requirements\\\"\\n            ]\\n        },\\n        {\\n            \\\"category\\\": \\\"üö´ Avoid Over-Indexing\\\",\\n            \\\"items\\\": [\\n                \\\"Don't index properties that are never queried\\\",\\n                \\\"Avoid indexes on highly variable properties (timestamps, UUIDs)\\\",\\n                \\\"Remove indexes on properties that are always used with other indexed properties\\\"\\n            ]\\n        },\\n        {\\n            \\\"category\\\": \\\"üìä Monitoring\\\",\\n            \\\"items\\\": [\\n                \\\"Regularly check index usage statistics\\\",\\n                \\\"Monitor query performance after index changes\\\", \\n                \\\"Review slow query logs to identify missing indexes\\\"\\n            ]\\n        }\\n    ]\\n    \\n    for rec in recommendations:\\n        print(f\\\"\\\\n   {rec['category']}:\\\")\\n        for item in rec['items']:\\n            print(f\\\"     ‚Ä¢ {item}\\\")\\n    \\n    # Demo 4: Composite index example\\n    print(\\\"\\\\n4Ô∏è‚É£ Advanced: Composite Index Example:\\\")\\n    print(\\\"   üí° For queries that filter on multiple properties\\\")\\n    \\n    composite_example = \\\"\\\"\\\"\\n    -- Example: Queries that filter by both person name and birth year\\n    MATCH (p:Person) \\n    WHERE p.name = 'Tom Hanks' AND p.born = 1956 \\n    RETURN p\\n    \\n    -- Optimized with composite index:\\n    CREATE INDEX person_name_born_index IF NOT EXISTS \\n    FOR (p:Person) ON (p.name, p.born)\\n    \\\"\\\"\\\"\\n    \\n    print(f\\\"   üìù Use Case:{composite_example}\\\")\\n    print(\\\"   ‚ö° Benefits: Single index lookup instead of multiple index intersections\\\")\\n    print(\\\"   üìè Trade-off: Larger index size, useful only for queries using both properties\\\")\\n    \\n    print(\\\"\\\\nüéì Key Takeaways:\\\")\\n    takeaways = [\\n        \\\"Indexes dramatically improve query performance but use storage space\\\",\\n        \\\"Create indexes on properties used in MATCH, WHERE, and ORDER BY clauses\\\",\\n        \\\"Use EXPLAIN/PROFILE to verify index usage in your queries\\\",\\n        \\\"Regular maintenance: monitor, review, and cleanup unused indexes\\\",\\n        \\\"Balance performance gains vs. storage/maintenance costs\\\"\\n    ]\\n    \\n    for takeaway in takeaways:\\n        print(f\\\"   ‚úÖ {takeaway}\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "appendix",
   "metadata": {},
   "source": [
    "## Appendix: Quick Reference\n",
    "\n",
    "### üîñ Cypher Quick Reference\n",
    "\n",
    "| Pattern | Description | Example |\n",
    "|---------|-------------|--------|\n",
    "| `(n)` | Any node | `MATCH (n) RETURN n` |\n",
    "| `(n:Label)` | Node with label | `MATCH (p:Person) RETURN p` |\n",
    "| `(n {prop: \"value\"})` | Node with property | `MATCH (p {name: \"Alice\"}) RETURN p` |\n",
    "| `-[:TYPE]->` | Directed relationship | `MATCH (a)-[:KNOWS]->(b) RETURN a, b` |\n",
    "| `-[:TYPE*1..3]-` | Variable length path | `MATCH (a)-[:KNOWS*1..3]-(b) RETURN a, b` |\n",
    "| `OPTIONAL MATCH` | Optional pattern | `OPTIONAL MATCH (p)-[:ACTED_IN]->(m)` |\n",
    "| `WHERE` | Filter condition | `WHERE p.age > 30` |\n",
    "| `WITH` | Pass data between clauses | `WITH p, count(*) AS connections` |\n",
    "| `ORDER BY` | Sort results | `ORDER BY p.name ASC` |\n",
    "| `LIMIT` | Restrict count | `LIMIT 10` |\n",
    "| `COLLECT()` | Aggregate to list | `COLLECT(p.name)` |\n",
    "| `COUNT()` | Count items | `COUNT(p)` |\n",
    "| `CASE WHEN` | Conditional logic | `CASE WHEN p.age > 30 THEN \"Adult\"` |\n",
    "\n",
    "### üìÅ Data Files Structure\n",
    "\n",
    "The tutorial uses these CSV files in the `data/` directory:\n",
    "- `movies.csv`: title, released, tagline\n",
    "- `people.csv`: name, born\n",
    "- `genres.csv`: name\n",
    "- `acted_in.csv`: actor, movie, role\n",
    "- `directed.csv`: director, movie\n",
    "- `movie_genres.csv`: movie, genre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}